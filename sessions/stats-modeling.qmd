# What are statistical models? {#sec-stats-modeling}


When working with data in research it is almost impossible to avoid
needing to do some form of statistical analysis. Running statistical
analyses is usually methodical and well-defined, though it often
involves trial and error. As it involves several steps: choosing the
right analysis to do for your data, ensuring that your data is
transformed in the right way to do your chosen statistical analysis, and
lastly, extracting the results of your statistical analysis so you can
present them. Sometimes it may feel overwhelming and complicated, which
it definitely can be, but it doesn't have to be. By taking a structured
approach, you can make the process easier and feel more in control.

In R, statistical methods are often developed by researchers with little
software training, which can make learning new methods challenging.
However, having a framework for running analyses can simplify the
process.

For this session, we are going to learn how we can generally set up and
run statistical models, so that we can use it within `{targets}`.

So this session is about running a basic statistical analysis for one
single research question, as a statistical model. The next session will
be taking what we've done here and applying it to run many at once.

## Learning objectives

{{< include /includes/objectives/_stats-modeling.qmd >}}

Specific "anti"-objectives on things that we will *not* learn:

-   How to choose and apply the appropriate statistical model or tests.
-   Statistical theory.
-   How to determine relevant data transformations for statistical
    tests.
-   How to interpret the statistical results.

This means that what we show is *for demonstration purposes only*. Our
choice of statistical model and analyses is likely wrong if an expert
were to review them.

## :book: Reading task: A brief introduction to statistical models and analyses

**Time: \~15 minutes.**

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Let them read it over than go over it briefly, focusing on what a model
is, that we should create research questions while thinking in the
framework of models, and the general workflow for doing statistical
analyses.
:::

::: {.callout-note collapse="true"}
## :teacher: Instructor note

A few things to repeat and reinforce:

1.  The workflow of the image and that it all starts with the research
    question.
2.  The fact that almost all statistical methods are basically special
    forms of linear regression.
3.  That this model creation stage requires a variety of domain
    expertise, not just statistical expertise.

Also repeat the question to ask, the outputs, as well as the functions
and their flow that we can translate into the `{targets}` pipeline.
:::

Research, especially quantitative research, is about making inferences
about the world by quantifying uncertainty from some data. And we
quantify uncertainty by using statistics and statistical models.

A statistical model is a simple way to describe the real world using
mathematics. In research, we often aim to understand the relationships
between multiple variables. For example, if we change the predictor
(independent variable or $x$), how does that affect the outcome
(dependent variable or $y$)?

Some simple examples are:

-   "How does this new cancer treatment affect survival rates?"
-   "How does this new user interface affect user watch time?"

or more complex:

-   "How does sunlight and water affect plant growth?"
-   "What is the relationship between a metabolite and type 1 diabetes
    compared to controls, adjusting for age and gender?"

These relationships can involve single or multiple predictors (such as
sunlight and water, or metabolite, gender, and age). The outcomes can be
either continuous (such as plant growth) or categorical (such as whether
a person has type 1 diabetes or not).

The relationship between predictors and the outcome is known as a
regression, or rather a *regression function*. So if there is a
relationship between $x$ and $y$, the math formula is:

$$y = f(x)$$

So for any given value of $x$, the function $f(x)$ will give us the
expected value of $y$. Since no relationship is perfect, we need to add
some random error ($error$) to describe the difference between the
function and the actual measured variables:

$$y = f(x) + error$$

The simplest form of regression is linear regression, where the formula
is a linear combination:

$$y = intercept + beta \times x + error$$

Here, the $intercept$ is the value of $y$ when $x$ is zero and $beta$ is
the value that describes how much $y$ changes for each unit increase in
$x$ (the "slope" of the relationship).

For example, we might expect a relationship between plant growth, water,
and sunlight. We could model plant growth as depending on water and
sunlight. Graphically, we can illustrate this model as:

```{mermaid fig-model-plant-growth}
%%| fig-cap: Simple example of a theoretical model of plant growth.
%%| echo: false
%%| eval: true
graph LR
    Sunlight --> Growth
    Water --> Growth
    Sunlight --- Water
```

or mathematically:

$$growth = sunlight + water$$

This math formula is our theoretical model that represents our research
question. However, not all theoretical models can be tested against the
real world. We need to use parameters (like sunlight) in our theoretical
model that can actually be measured. In this case, we need to measure
plant growth (e.g., weight in grams), the amount of water given per day
(in liters), and the number of sunlight hours per day.

Why is it important to think about research questions in terms of
models? Aside from helping you better design your research questions and
your research study that can answer the questions, it also helps with
how to structure your code for running statistical analyses. Knowing how
the statistical model mathematically works means you can write code that
effectively and efficiently runs your analyses in a reproducible and
simpler way.

::: {.callout-note collapse="true"}
## :teacher: Instructor note

A few things to repeat and reinforce:

1.  The workflow of the image and that it all starts with the research
    question.
2.  The fact that almost all statistical methods are basically special
    forms of linear regression.
3.  That this model creation stage requires a variety of domain
    expertise, not just statistical expertise.

Also repeat the question to ask, the outputs, as well as the functions
and their flow that we can translate into the `{targets}` pipeline.
:::

It also means that there's a generic process to going from research
question to getting results that we can apply to many types of analyses,
leading to a common "pattern" for writing code. This process generally
involves the following steps:

1.  **Write a research question** that fits a theoretical model with
    measurable parameters.

2.  **Select the best model type** based on the model parameters (e.g.,
    linear regression, logistic regression, ANOVA, or t-test).

3.  **Collect or select the right data for your model parameters**
    (e.g., water in liters per day, plant growth in weight, sunlight in
    hours per day). Collect data if you don't have any or select from
    existing data.

4.  **Fit the data to the model** to estimate the values (coefficients)
    of the model, such as the intercept, slope, and uncertainty (error).

5.  **Extract and present the values** in relation to your research
    questions.

```{mermaid fig-model-building-workflow}
%%| fig-cap: Simple schematic of the process for conducting statistical analysis.
%%| echo: false
%%| eval: true
graph TD
    A[Research question] --> B[Theoretical model]
    B --> A
    B --> C[Data collection<br>or selection]
    C --> D[Data transformation]
    D --> F[Fit model<br>to data]
    B --> F
    F --> E[Scientific results]
    E --> A
```

::: {.callout-caution appearance="default"}
Connecting the correct statistical model to the data and research
question requires highly specific domain knowledge in multiple areas.
For instance, in our `lipidomics` dataset, if we were to actually
analyse this data, we would need experts who are familiar with, e.g. how
-omic data are measured, how to transform them, which modeling methods
to use, and how to interpret the results. We have **none** of these
things, so we're likely doing things incorrectly. But since this is a
workshop on coding and reproducible, what we will do is only for
demonstration purposes.
:::

##

```{mermaid}
flowchart TB
    q[Question] -->|?| result[Result]
```

```{mermaid}
flowchart TB
    q[Question] -->|?| stats[Statistics]
    stats -->|?| result[Result]
```

```{mermaid}
flowchart TB
    q[Question] -->|?| model[Statistical<br>model]
    model & data[Data] -->|?| result[Result]
```

Furthermore, because we are working within a "reproducible analysis"
framework specifically with the use of `{targets}`, we will also convert
questions regarding our lipidomics data into outputs to include as
pipeline targets, along with a basic idea for the final functions that
will make up these targets and their inputs and outputs. These targets
will probably be quite different by the end, but it's a good start to
think about what it should look like in the end.

-   All results for estimated relationships (in case we want to use it
    for other output)
-   Plot of statistical estimate for each relationship

Potential function names might be:

-   `calculate_estimates()`
-   `plot_estimates()`


```{mermaid}
%%| label: fig-stats-analysis-pipeline
%%| fig-cap: Targets in our pipeline that we've done in the last session and those we will do in this session.
%%| echo: false
graph TB
    data_file["data_file<br>'data/lipidomics.csv'"]:::done -->|"read_csv()"| lipidomics:::done
    lipidomics -->|"create_model_results()"| model_results:::wip
    model_results -->|"plot_model_results()"| plot_model_results:::wip
    model_results -->|"tar_read()"| report:::wip
    plot_model_results -->|"tar_read()"| report:::wip
    classDef done stroke-width:3px
    classDef wip fill:#ffedd6
```

{{< text_snippet sticky_up >}}


## :book: Reading task: Defining a model for our lipidomics data

**Time: \~10 minutes.**

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Let them read it first and then briefly verbally walk through this
section, describing the theoretical model both graphically and
mathematically. Go through why we use `{tidymodels}` rather than other
approaches.
:::

Going back to our own lipidomics dataset, we need to do the first step:
Creating the question. While we don't have much data, there are a
surprising number of questions we could ask. But we will keep it very
simple, very basic, and very exploratory.

1.  What is the estimated relationship of each metabolite with type 1
    diabetes (T1D) compared to the controls, adjusting for the influence
    of age and gender?

A graphical representation of this theoretical model could be:

```{mermaid fig-model-research-question}
%%| fig-cap: A simple theoretical model of the research question about T1D status.
%%| echo: false
%%| eval: true
graph TB
    Metabolite --> T1D
    Age & Gender --> T1D & Metabolite
```

Or mathematically:

$$T1D = metabolite + age + gender$$

So, T1D status (or `class` in the `lipidomics` dataset) is our
**outcome** and the individual metabolite, age, and gender are our
**predictors**. Technically, age and gender would be "confounders" or
"covariates", since we include them only because we think they influence
the relationship between the metabolite and T1D.

If we convert the formula into a form that uses the variable names that
we have in the dataset, it would look like:

$$class = metabolite + age + gender$$

However, in this particular case, `metabolite` contains all the
metabolites, so we'll have to keep only the metabolite we are interested
in for the particular model we want to run.

```{mermaid}
flowchart TB
    q[Question] --> model[Model]
    model & data[Data] -->|?| rs[Results]
```

split (only works with small to medium sized data, e.g. when your data
can't fit into your computers memory), transform data, fit model,
extract results, join.

Now that we have a theoretical model, we need to choose our model type.
Since T1D is binary (either you have it or you don't), the most likely
choice is logistic regression, which requires a binary outcome variable.
So we have the theoretical model and the type of model to use, now how
do we express this as code in R? There are many ways of doing the same
thing in R, but some are a bit easier than others.

##

Let's switch to working in the `doc/learning.qmd` file to create those
logistic regression models. In the `doc/learning.qmd` file, on the
bottom of the document create a new header and code chunk:

```` {.markdown filename="docs/learning.qmd"}
## Building the model

```{{r}}

```
````

In the new code chunk, we will set up the model formula:

```{r}
data <- lipidomics |>
  filter(metabolite == "Cholesterol") |>
  group_by(pick(-value)) |>
  summarise(value = mean(value, na.rm = TRUE), .groups = "keep") |>
  ungroup() |>
  mutate(
    class = as.factor(class),
    value = scale(value)
  )
```

```{r}
glm(
  formula = class ~ value + age + gender,
  data = data,
  family = binomial
) |>
  broom::tidy(exponentiate = TRUE)

model <- class ~ value + age + gender

glm(
  formula = class ~ value + age + gender,
  data = data,
  family = binomial
) |>
  broom::tidy(exponentiate = TRUE)

preprocess <- function(data) {
  data |>
    filter(metabolite == "Cholesterol") |>
    group_by(pick(-value)) |>
    summarise(value = mean(value, na.rm = TRUE), .groups = "keep") |>
    ungroup() |>
    mutate(
      class = as.factor(class),
      value = scale(value)
    )
}

fit_model <- function(data, model) {
  glm(
    formula = model,
    data = data,
    family = binomial
  ) |>
    broom::tidy(exponentiate = TRUE) |>
    dplyr::mutate(model = as.character(model))
}

lipidomics |>
  preprocess() |>
  fit_model(class ~ value + age + gender)
```

## Data transformations specific to modeling

Setting the model type was pretty easy right? The more difficult part
comes next with the data transformations.

Let's consider our `lipidomics` dataset. In order for us to start our
statistical analysis, we need the data to be structured in a certain way
to be able to smoothly use it as input in our model. We have at least
three necessary transformations.

(As exercise?)

```{r}
#| filename: Console
lipidomics
```


The first issue, which isn't always an issue, depends heavily on the
model type you use. Since we are using logistic regression, the model
assumes that each row is an individual person. But our data is in the
long format, so each person has multiple rows per metabolite.
But let's confirm if each person has one value per metabolite:


```{r too-many-cholesterols}
#| filename: "docs/learning.qmd"
lipidomics |>
  count(code, metabolite) |>
  filter(n > 1)
lipidomics |>
  group_by(-value) |>
  summarise(value = mean(value))
```

The second issue is that
there seems to be a data entry error, since there are multiple
`Cholesterol` values, while all other metabolites only have one:

To actually fix the multiple
cholesterol issue, we should look more into the data documentation or
contact the authors. But for this workshop, we will summarise all
the metabolites by mean. In this case, only Cholesterol will be summarised, all the
rest values won't be impacted.

```{mermaid}
%%| label: fig-stats-analysis-pipeline
%%| fig-cap: Targets in our pipeline that we've done in the last session and those we will do in this session.
%%| echo: false
graph TB
    data_file["data_file<br>'data/lipidomics.csv'"]:::done -->|"read_csv()"| lipidomics:::done
    lipidomics -->|"clean()"| tidy_lipidomics:::wip
    tidy_lipidomics -->|"create_model_results()"| model_results:::wip
    model_results -->|"plot_model_results()"| plot_model_results:::wip
    model_results -->|"tar_read()"| report:::wip
    plot_model_results -->|"tar_read()"| report:::wip
    classDef done stroke-width:3px
    classDef wip fill:#ffedd6
```





## Pre-processing before modeling

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Verbally talk through the next few paragraphs, no code-along yet.
:::

How to make the
results between metabolites comparable.

Each
metabolite has quite large differences in the values and ranges of data.
Again, whether this is an issue depends on what we want to do, but in
our research question we want to know how each metabolite influences
T1D. In order to best interpret the results and compare across
metabolites, we should ideally have all the metabolites with a similar
range and distribution of values.

There are many transformations we could use for the `lipidomics`
dataset, but we will use normalization (which is a combination of mean-centering and scaling) for this workshop. This
is useful because it makes each variable centered to zero at the mean and a value of
1 unit is translated to 1 standard deviation of the original
distribution. This means we can more easily compare values between
variables.

Style using {{< var keybind.styler >}}, then commit the changes made to
the Git history with {{< var keybind.git >}}.

## Fitting the model to the data {#sec-fitting-model}

We've now defined the model we want to use and transformed the data. Now
we can start putting them together and finally fit them to the data.

To get this information in a tidier format, we use another function:
`tidy()`. This function comes from the `{broom}` package. But we should explicitly add it to the
dependencies:

```{r broom-to-deps}
#| purl: true
#| eval: false
#| filename: Console
use_package("broom")
```

Then, we add the `tidy()` function to our model using the `|>` pipe.
Since we are using a logistic regression model, we need to consider how
we want the estimates to be presented, probably depending on how we want
to visualize our results. If we set `exponentiate = TRUE` in `tidy()`,
the output estimates will be odds ratios, if we set
`exponentiate = FALSE`, we will get the log odds ratios. Here we choose
`exponentiate = TRUE`:

```{r tidy-up-model-results}
#| filename: "docs/learning.qmd"
fitted_model |>
  tidy(exponentiate = TRUE)
```

We now have a data frame of our model results!

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Reinforce **not** to worry about interpreting the results, that is not
the aim of this workshop.
:::

::: callout-important
Another reminder, we **are not** interpreting this results. For this
workshop, they are **not** important and can distract from the main
purpose.
:::

Let's briefly cover what these columns and values mean. But first, open
the Git interface and commit the changes you made with
{{< var keybind.git >}}. Then push your changes up to GitHub.

::: {.callout-note collapse="true"}
## :teacher: Instructor note

If you want, you can go over these details briefly or in more detail,
depending on how comfortable you are. Or you can get them to read it
only.
:::

## :technologist: Exercise: Add a target for the model results

**Time: \~8 minutes.**

Taking the function code from above, you'll next add the model results
output to end of the `_targets.R` file, using the below scaffold as a
guide.

``` r
list(
  ...,
  tar_target(
    name = ___,
    command = ___(___)
  )
)
```

1.  Use `df_model_estimates` for the `name`.
2.  Use the `calculate_estimates()` function in `command`, with
    `lipidomics` as the argument.
3.  Use {{< var keybind.styler >}} to style and than run
    `targets::tar_visnetwork()` using {{< var keybind.targets-vis >}} to
    see if the new target gets detected. If it does, than run
    `targets::tar_make()` with {{< var keybind.targets-make >}}.
4.  Commit the changes to the Git history with {{< var keybind.git >}}.

```{r solution-target-model-estimates}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
list(
  # ...,
  tar_target(
    name = df_model_estimates,
    command = calculate_estimates(lipidomics)
  )
)
```

```{r purl-only-model-estimates-to-targets}
#| eval: false
#| echo: false
#| purl: true
update_target_calc_est <- "
  ),
  list(
    name = df_model_estimates,
    command = calculate_estimates(lipidomics)
  )
)
"
# print_lines("_targets.R")
revise_by_line_num(
  path = "_targets.R",
  insert_text = update_target_calc_est,
  # Modify these numbers.
  remove_original_lines = -30,
  insert_at_line = 31
)
targets::tar_visnetwork()
targets::tar_make()
git_ci("_targets.R", "Update targets with model results")
```

{{< text_snippet sticky_up >}}

## :book: Reading task: Understanding the model output

**Time: \~10 minutes.**

Let's explain this output a bit, each column at a time:

-   `term`: If you recall the formula
    $class = metabolite + age + gender$, you'll see all but the `class`
    object there in the column `term`. This column contains all the
    predictor variables, including the intercept (from the original
    model).

-   `estimate`: This column is the "coefficient" linked to the term in
    the model. The final mathematical model here looks like:

    $$ \displaylines{class = Intercept + (metabolite\_estimate \times metabolite\_value) + \\ (gender\_estimate \times gender\_value) + ...}$$

    In our example, we chose to get the odds ratios. In the mathematical
    model above, the estimate is represented as the log odds ratio or
    beta coefficient - the constant value you multiply the value of the
    term with. Interpreting each of these values can be quite tricky and
    can take a surprising amount of time to conceptually break down, so
    we won't do that here, since this isn't a statistics workshop. The
    only thing you need to understand here is that the `estimate` is the
    value that tells us the *magnitude* of association between the term
    and `class`. This value, along with the `std.error` are the most
    important values we can get from the model and we will be using them
    when presenting the results.

-   `std.error`: This is the uncertainty in the `estimate` value. A
    higher value means there is less certainty in the value of the
    `estimate`.

-   `statistic`: This value is used to, essentially, calculate the
    `p.value`.

-   `p.value`: This is the infamous value we researchers go crazy for
    and think nothing else of. While there is a lot of attention to this
    single value, we tend to give it more attention than warranted. The
    interpretation of the p-value is even more difficult than the
    `estimate` and again, we won't cover this in this workshop. We won't
    be using this value at all in presenting the results.

{{< text_snippet sticky_up >}}

## Summary

-   Create research questions that (ideally) are structured in a way to
    mimic how the statistical analysis will be done, preferably in a
    "formula" style like $y = x1 + x2 + ... + error$ and in a diagram
    style with links connecting variables.
-   Statistical analyses, while requiring some trial and error, are
    surprisingly structured in the workflow and steps taken. Use this
    structure to help guide you in completing tasks related to running
    analyses.
-   Use `{broom}` to `tidy()` the model output, extracted using
    `extract_fit_parsnip()` to get a data frame of the estimates and
    standard error for the variables in the model.

{{< include /includes/_code-appendix.qmd >}}
