# Efficiently running many analyses at once {#sec-stats-analyses-multiple}

```{r setup}
#| include: false
source(here::here("R/functions.R"))
extract_functions_from_qmd()
source(here::here("R/project-functions.R"))
library(tidyverse)
library(tidymodels)
lipidomics <- read_csv(here::here("data/lipidomics.csv"))
lipidomics_wide <- lipidomics |>
  mutate(metabolite = snakecase::to_snake_case(metabolite)) |>
  metabolites_to_wider()
# lipidomics |>
#   calculate_estimates() |>
#   readr::write_csv(here::here("data/model-estimates.csv"))
```

::: {.callout-note collapse="true"}
## Instructor note

Before beginning, get them to recall what they remember of the previous
session, either with something like Mentimeter or verbally. Preferably
something like Mentimeter because it allows everyone to participate, not
just the ones who are more comfortable being vocal to the whole group.

Depending on what they write, might need to briefly go over the previous
session.
:::

Rarely do we run only one single statistical model to answer one single
question, especially in our data-overflowing environments. An initial
instinct when faced with this task might be to copy-and-paste, then
slightly modify the code each time. Or, if you have heard of loops or
used them in other programming languages, you might think to create a
loop. Thankfully R uses something more powerful and expressive than
either of those approaches, and that is functional programming. Using
functional programming concepts, we can use little code to express
complex actions and run large numbers of statistical analyses. This
session will be about using functional programming in the context of
statistical analysis.

## Learning objectives {#sec-stats-analyses-multiple-learning-objectives}

The overall objective for this session is to:

1.  Describe the basic framework underlying most statistical analyses
    and use R to generate statistical results using this framework.

More specific objectives are to:

1.  Recall principles of functional programming and apply them to
    running statistical analyses by using the `{purrr}` package.
2.  Continue applying the concepts and functions used from the previous
    sessions.

Specific "anti"-objectives:

-   Same as the "anti"-objectives of @sec-stats-analyses-basic.

## Apply logistic regression to each metabolite with functional programming

::: callout-note
## :book: Reading task: \~10 minutes

Functional programming underlies many core features of running
statistical methods on data. Because it is such an important component
of this session, you'll briefly review the concepts of functional
programming by going to the [Function
Programming](https://r-cubed-intermediate.rostools.org/sessions/functionals#functional-programming)
in the Intermediate R course (**only the reading task section**) as well
as the
[split-apply-combine](https://r-cubed-intermediate.rostools.org/sessions/functionals#split-apply-combine-technique-and-functionals)
(this is a short section).
:::

::: {.callout-note collapse="true"}
## Instructor note

Reinforce the concept of functional programming by briefly going over
the
[figure](https://r-cubed-intermediate.rostools.org/sessions/functionals#fig-functionals)
that visualizes functionals like `map()`.
:::

There are many ways that you can run a model on each metabolite based on
the `lipidomics_wide` dataset. However, these types of
"split-apply-combine" tasks are (usually) best done using data in the
long form. So we'll start with the original `lipidomics` dataset. Create
a header and code chunk at the end of the `doc/learning.qmd` file:

````{.markdown filename="doc/learning.qmd"}
## Running multiple models

```{{r}}

```
````

The first thing we want to do is convert the metabolite names into snake
case:

```{r chain-col-to-snakecase}
#| filename: "doc/learning.qmd"
lipidomics |>
  column_values_to_snake_case(metabolite)
```

The next step is to split the data up. We could use `group_by()`, but in
order to make the most use of `{purrr}` functions like `map()`, we will
use `group_split()` to convert the data frame into a set of
lists[^stats-analyses-multiple-1]. Let's first add `{purrr}` as a
dependency:

[^stats-analyses-multiple-1]: There is probably a more computationally
    efficient way of coding this instead of making a list, but as the
    saying goes ["premature optimization is the root of all
    evil"](https://stackify.com/premature-optimization-evil/). For our
    purposes, this is a very good approach, but for very large datasets
    and hundreds of potential models to run, this method would need to
    be optimized some more.

```{r purrr-to-deps}
#| purl: true
#| eval: false
#| filename: Console
use_package("purrr")
```

Then we run `group_split()` on the `metabolite` column, which will
output a lot of data frames as a list. The website only shows the first
three.

```{r chain-split-by-metabolite}
#| eval: false
#| filename: "doc/learning.qmd"
lipidomics |>
  column_values_to_snake_case(metabolite) |>
  group_split(metabolite)
```

```{r output-only-chain-split-by-metabolite}
#| echo: false
lipidomics |>
  column_values_to_snake_case(metabolite) |>
  group_split(metabolite) |>
  head(3)
```

Remember that logistic regression models need each row to be a single
person, so we'll use the functional `map()` to apply our custom function
`metabolites_to_wider()` on each of the split list items (only showing
the first three):

```{r chain-map-to-wider}
#| eval: false
#| filename: "doc/learning.qmd"
lipidomics |>
  column_values_to_snake_case(metabolite) |>
  group_split(metabolite) |>
  map(metabolites_to_wider)
```

```{r output-only-chain-map-to-wider}
#| echo: false
lipidomics |>
  column_values_to_snake_case(metabolite) |>
  group_split(metabolite) |>
  map(metabolites_to_wider) |>
  head(3)
```

Alright, we now a list of data frames where each data frame has only one
of the metabolites. These bits of code represent the conceptual action
of "splitting the data into a list by metabolites". Since we're
following a function-oriented workflow, let's create a function for
this. Convert it into a function, add the Roxygen documentation using
{{< var keybind.roxygen >}} style using {{< var keybind.styler >}}, move
into the `R/functions.R` file, and then `source()` the file with
{{< var keybind.source >}}.

```{r new-function-split-by-metabolite}
#| filename: "R/functions.R"
#' Convert the long form dataset into a list of wide form data frames.
#'
#' @param data The lipidomics dataset.
#'
#' @return A list of data frames.
#'
split_by_metabolite <- function(data) {
  data |>
    column_values_to_snake_case(metabolite) |>
    dplyr::group_split(metabolite) |>
    purrr::map(metabolites_to_wider)
}
```

In the `doc/learning.qmd`, use the new function in the code:

```{r split-by-metabolite}
#| filename: "doc/learning.qmd"
#| eval: false
lipidomics |>
  split_by_metabolite()
```

```{r output-only-split-by-metabolite}
#| echo: false
lipidomics |>
  split_by_metabolite() |>
  head(3)
```

Like we did with the `metabolite_to_wider()`, we need to pipe the output
into another `map()` function that has a custom function running the
models. We don't have this function yet, so we need to create it. Let's
convert the modeling code we used in the code at the end of section
@sec-fitting-model into a function, replacing `lipidomics` with `data`
and using `starts_with("metabolite_")` within the
`create_recipe_spec()`. Add the Roxygen documentation using
{{< var keybind.roxygen >}}, use {{< var keybind.styler >}} to style,
move into the `R/functions.R` file, and then `source()` the file with
{{< var keybind.source >}}.

```{r new-function-generate-model-results}
#| filename: "R/functions.R"
#' Generate the results of a model
#'
#' @param data The lipidomics dataset.
#'
#' @return A data frame.
#'
generate_model_results <- function(data) {
  create_model_workflow(
    parsnip::logistic_reg() |>
      parsnip::set_engine("glm"),
    data |>
      create_recipe_spec(tidyselect::starts_with("metabolite_"))
  ) |>
    parsnip::fit(data) |>
    tidy_model_output()
}
```

Then we add it to the end of the pipe, but using `map()` and
`list_rbind()` to convert to a data frame:

```{r chain-generate-model-results}
#| filename: "doc/learning.qmd"
lipidomics |>
  split_by_metabolite() |>
  map(generate_model_results) |>
  list_rbind()
```

Since we are only interested in the model results for the metabolites,
let's keep only the `term` rows that are metabolites using `filter()`
and `str_detect()`.

```{r chain-filter-terms}
#| filename: "doc/learning.qmd"
model_estimates <- lipidomics |>
  split_by_metabolite() |>
  map(generate_model_results) |>
  list_rbind() |>
  filter(str_detect(term, "metabolite_"))
model_estimates
```

Wow! We're basically at our first `{targets}` output! Before continuing,
there is one aesthetic thing we can add: The original variable names,
rather than the snake case version. Since the original variable still
exists in our `lipidomics` dataset, we can join it to the
`model_estimates` object with `right_join()`, along with a few other
minor changes. First, we'll `select()` only the `metabolite` and then
create a duplicate column of `metabolite` called `term` (to match the
`model_estimates`) using `mutate()`.

```{r duplicate-original-vars}
#| filename: "doc/learning.qmd"
lipidomics |>
  select(metabolite) |>
  mutate(term = metabolite)
```

Right after that we will use our custom `column_values_to_snake_case()`
function on the `term` column.

```{r dup-column-to-snakecase}
#| filename: "doc/learning.qmd"
lipidomics |>
  select(metabolite) |>
  mutate(term = metabolite) |>
  column_values_to_snake_case(term)
```

We can see that we are missing the `metabolite_` text before each snake
case'd name, so we can add that with `mutate()` and `str_c()`:

```{r dup-column-append-metabolite}
#| filename: "doc/learning.qmd"
lipidomics |>
  select(metabolite) |>
  mutate(term = metabolite) |>
  column_values_to_snake_case(term) |>
  mutate(term = str_c("metabolite_", term))
```

There are 504 rows, but we only need the unique values of `term` and
`metabolite`, which we can get with `distinct()`. Because we will use
`distinct()`, we don't need to use `select()` as well, since it keeps
only the `metabolite` and `term` variables.

```{r dup-column-distinct}
#| filename: "doc/learning.qmd"
lipidomics |>
  mutate(term = metabolite) |>
  column_values_to_snake_case(term) |>
  mutate(term = str_c("metabolite_", term)) |>
  distinct(term, metabolite)
```

The last step is to `right_join()` with the `model_estimates`:

```{r dup-column-full-join}
#| filename: "doc/learning.qmd"
lipidomics |>
  mutate(term = metabolite) |>
  column_values_to_snake_case(term) |>
  mutate(term = str_c("metabolite_", term)) |>
  distinct(term, metabolite) |>
  right_join(model_estimates, by = "term")
```

Awesome :smile: Now can you guess what we are going to do next? That's
right, making a function of both the model creation code and this code
to add the original variable names. Then we can add our first
`{targets}` output!

## :technologist: Exercise: Creating functions for model results and adding as a target in the pipeline

<!-- TODO: convert this into two exercises -->

> Time: \~25 minutes.

::: {.callout-warning appearance="default"}
There are two parts to this exercise and two solution chunks.
:::

**Part 1**: Convert the code that calculates the model estimates as well
as the code that adds the original metabolite names into functions.
Start with the code for the metabolite names, using the scaffold below
as a starting point.

1.  Name the new function `add_original_metabolite_names`.
2.  Within the `function()`, add two arguments, where the first is
    called `model_results` and the second is called `data`.
3.  Paste the code we created into the function, replacing `lipidomics`
    with `data` and `model_estimates` with `model_results`.
4.  Add `dplyr::` and `stringr::` before their respective functions.
5.  Add the Roxygen documentation using {{< var keybind.roxygen >}}.
6.  Use {{< var keybind.styler >}} to style the file to fix up the code.
7.  Cut and paste the function over into the `R/functions.R` file.
8.  Commit the changes you've made so far with {{< var keybind.git >}}.

``` r
___ <- function(___, ___) {
  ___ |>
}
```

```{r solution-new-function-add-original-metabolite-names}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
#' Add the original metabolite names (not as snakecase) to the model results.
#'
#' @param model_results The data frame with the model results.
#' @param data The original, unprocessed lipidomics dataset.
#'
#' @return A data frame.
#'
add_original_metabolite_names <- function(model_results, data) {
  data |>
    dplyr::mutate(term = metabolite) |>
    column_values_to_snake_case(term) |>
    dplyr::mutate(term = stringr::str_c("metabolite_", term)) |>
    dplyr::distinct(term, metabolite) |>
    dplyr::right_join(model_results, by = "term")
}
```

**Part 2**: Do the same thing with the code that creates the model
results, using the scaffold below as a starting point.

``` r
calculate_estimates <- function(data) {
  ___ |>
    # All the other code to create the results
    ___ |> 
    add_original_metabolite_names(data) 
}
```

```{r solution-new-function-calculate-estimates}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
#' Calculate the estimates for the model for each metabolite.
#'
#' @param data The lipidomics dataset.
#'
#' @return A data frame.
#'
calculate_estimates <- function(data) {
  data |>
    split_by_metabolite() |>
    purrr::map(generate_model_results) |>
    purrr::list_rbind() |>
    dplyr::filter(stringr::str_detect(term, "metabolite_")) |>
    add_original_metabolite_names(data)
}
```

Lastly, add the model results output to end of the `_targets.R` file,
using the below scaffold as a guide.

1.  Use `df_model_estimates` for the `name`.
2.  Use the `calculate_estimates()` function in `command`, with
    `lipidomics` as the argument.
3.  Use {{< var keybind.styler >}} to style and than run
    `targets::tar_visnetwork()` using {{< var keybind.targets-vis >}} to
    see if the new target gets detected. If it does, than run
    `targets::tar_make()` with {{< var keybind.targets-make >}}.
4.  Commit the changes to the Git history with {{< var keybind.git >}}.

``` r
list(
  ...,
  tar_target(
    name = ___,
    command = ___(___)
  )
)
```

```{r purl-only-model-estimates-to-targets}
#| eval: false
#| echo: false
#| purl: true
update_target_calc_est <- "
  ),
  list(
    name = df_model_estimates,
    command = calculate_estimates(lipidomics)
  )
)
"
# print_lines("_targets.R")
revise_by_line_num(
  path = "_targets.R",
  insert_text = update_target_calc_est,
  # Modify these numbers.
  remove_original_lines = -30,
  insert_at_line = 31
)
targets::tar_visnetwork()
targets::tar_make()
git_ci("_targets.R", "Update targets with model results")
```

## Visualizing the model estimates

We've got one target done for the modeling stage, three more to go!
There are multiple ways of visualizing the results from models. A common
approach is to use a "dot-and-whisker" plot like you might see in a
meta-analysis. Often the "whisker" part is the measure of uncertainty
like the confidence interval, and the "dot" is the estimate. For the
confidence interval, we haven't calculated them at this point because
the typical approach doesn't exactly work for our data (tested before
the course). For this plot though, we will use the standard error of the
estimate.

Inside the `doc/learning.qmd`, let's create a new header and code chunk
inside the `## Results` section. We'll want to use
`tar_read(df_model_estimates)` so that `{targets}` is aware that the R
Markdown file is dependent on this target.

````{.markdown filename="doc/learning.qmd"}
### Figure of model estimates

```{{r}}
model_estimates <- tar_read(df_model_estimates)
```
````

```{r purl-only-model-estimate-md-text}
#| eval: false
#| echo: false
#| purl: true
model_estimate_md_text <- c(
  "### Figure of model estimates",
  "",
  "```{r}",
  "model_estimates <- tar_read(df_model_estimates)",
  "```"
)

print_file("doc/learning.qmd")
revise_by_line_num(
  "doc/learning.qmd",
  model_estimate_md_text,
  # Update numbers
  remove_original_lines = -40,
  insert_at_line = 39
)
git_ci("doc/learning.qmd", "Add code for model estimates to report.")
```

```{r exec-only-model-estimates}
#| include: false
model_estimates <- here::here("data/model-estimates.csv") |>
  readr::read_csv(show_col_types = FALSE)
```

Then we'll start using `{ggplot2}` to visualize the results. For
dot-whisker plots, the "geom" we would use is called
`geom_pointrange()`. It requires four values:

-   `x`: This will be the "dot", representing the `estimate` column.
-   `y`: This is the categorical variable that the "dot" is associated
    with, in this case, it is the `metabolite` column.
-   `xmin`: This is the lower end of the "whisker". Since the
    `std.error` is a value representing uncertainty of the estimate on
    either side of it (`+` or `-`), we will need to subtract `std.error`
    from the `estimate`.
-   `xmax`: This is the upper end of the "whisker". Like `xmin` above,
    but adding `std.error` instead.

```{r plot-estimates-pointrange-only}
#| filename: "doc/learning.qmd"
plot_estimates <- model_estimates |>
  ggplot(aes(
    x = estimate,
    y = metabolite,
    xmin = estimate - std.error,
    xmax = estimate + std.error
  )) +
  geom_pointrange()
plot_estimates
```

Woah, there is definitely something wrong here. The values of the
estimate should realistically be somewhere between 0 (can't have a
negative odds) and 2 (in biology and health research, odds ratios are
rarely above 2), definitely unlikely to be more than 5. We will
eventually need to troubleshoot this issue, but for now, let's restrict
the x axis to be between 0 and 5.

```{r plot-estimates-coord-fixed}
#| filename: "doc/learning.qmd"
plot_estimates +
  coord_fixed(xlim = c(0, 5))
```

There are so many things we could start investigating based on these
results in order to fix them up. But for now, this will do.

## :technologist: Exercise: Add plot function as a target in the pipeline

> Time: \~15 minutes.

Hopefully you've gotten comfortable with the function-oriented workflow,
because we'll need to convert this plot code into a function and add it
as a target in the pipeline. Use the scaffold below as a guide.

1.  Replace `model_estimates` with `results`.
2.  Move the function into the `R/functions.R` file, add the Roxygen
    documentation using {{< var keybind.roxygen >}}, and use
    {{< var keybind.styler >}} to style.

``` r
plot_estimates <- function(results) {
  ___ |> 
    # Plot code here:
    ___
}
```

```{r solution-new-function-plot-estimates}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
#' Plot the estimates and standard errors of the model results.
#'
#' @param results The model estimate results.
#'
#' @return A ggplot2 figure.
#'
plot_estimates <- function(results) {
  results |>
    ggplot2::ggplot(ggplot2::aes(
      x = estimate, y = metabolite,
      xmin = estimate - std.error,
      xmax = estimate + std.error
    )) +
    ggplot2::geom_pointrange() +
    ggplot2::coord_fixed(xlim = c(0, 5))
}
```

Then, after doing that, add the new function as a target in the
pipeline, name the new `name` as `fig_model_estimates`. Inside the
`plot_estimates()` function, use the the model estimate target we
created previously (`df_model_estimates`).

``` r
list(
  ...,
  tar_target(
    name = ___,
    command = plot_estimates(___)
  )
)
```

```{r solution-target-plot-estimates}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
list(
  # ...,
  tar_target(
    name = fig_model_estimates,
    command = plot_estimates(df_model_estimates)
  )
)
```

```{r purl-only-tar-make-fig-estimates}
#| eval: false
#| echo: false
#| purl: true
plot_estimates_md_text <- c(
  "",
  "```{{r}}",
  "tar_read(fig_model_estimates)",
  "```",
  ""
)

print_file("doc/learning.qmd")
revise_by_line_num(
  "doc/learning.qmd",
  plot_estimates_md_text,
  # Update numbers
  remove_original_lines = -40,
  insert_at_line = 39
)
git_ci("doc/learning.qmd", "Add code for model estimates to report.")

update_target_plot_est <- "),
tar_target(
  name = fig_model_estimates,
  command = plot_estimates(df_model_estimates)
)
)
"
# print_lines("_targets.R")
styler::style_file("_targets.R")
revise_by_line_num(
  path = "_targets.R",
  insert_text = update_target_calc_est,
  # Modify these numbers.
  remove_original_lines = -30,
  insert_at_line = 31
)
git_ci("_targets", "Add plot estimates target")
```

## Combine all the output into the Quarto file

Now its' time to add the model results and plots to the
`doc/learning.qmd` file. Open it up and create another code chunk at the
bottom of the file. Like we did with the other outputs (like the
figures), we'll use `tar_read()` to reference the image path.

```{{r}}
tar_read(fig_model_estimates)
```

Run `targets::tar_visnetwork()` using {{< var keybind.targets-vis >}},
then `targets::tar_make()` with {{< var keybind.targets-make >}}. We now
have the report rendered to an HTML file! If you open it up in a
browser, we can see the figures added to it. Before ending, commit the
changes to the Git history with {{< var keybind.git >}}.

<!-- TODO: Check that the output is rendered to html. Don't forget the TODO -->

```{r purl-only-tar-read-in-quarto}
#| eval: false
#| echo: false
#| purl: true
plot_estimates_md_text <- c(
  "",
  "```{{r}}",
  "tar_read(fig_model_estimates)",
  "```",
  ""
)

print_file("doc/learning.qmd")
revise_by_line_num(
  "doc/learning.qmd",
  plot_estimates_md_text,
  # Update numbers
  remove_original_lines = -40,
  insert_at_line = 39
)
git_ci("doc/learning.qmd", "Add code for model estimates to report.")
targets::tar_visnetwork()
targets::tar_make()
```

## Summary

-   Use functional programming with `map()`, as part of the
    function-oriented workflow, to run multiple models efficiently and
    with minimal code.
-   Consistently create small functions that do a specific conceptual
    action and chain them together into larger conceptual actions, which
    can then more easily be incorporated into a `{targets}` pipeline.
    Small, multiple functions combined together are easier to manage
    than fewer, bigger ones.
-   Use dot-whisker plots like `geom_pointrange()` to visualize the
    estimates and their standard error.
