---
execute:
    eval: false
---

# Efficiently running many analyses at once {#sec-stats-analyses-multiple}

```{r setup}
#| include: false
source(here::here("R/functions.R"))
extract_functions_from_qmd()
source(here::here("R/project-functions.R"))
library(tidyverse)
# To trigger downlit
library(purrr)
library(tidymodels)
lipidomics <- read_csv(here::here("data/lipidomics.csv"))
```

```{mermaid}
graph LR
    subgraph cat1 [List 1]
      A1(Model<br>formula 1)
      A2(Model<br>formula 2)
    end
    subgraph cat2 [List 2]
      B1(Results 1)
      B2(Results 2)
    end

    cat1:::functor -->|"map F(formula, data)"| cat2:::functor

    A1 -->|"f()"| B1
    A2 -->|"f()"| B2

    classDef functor fill:white;
```

{{< text_snippet review_note >}}

Rarely do we run only one single statistical model to answer one single
question, especially in our data-overflowing environments. An initial
instinct when faced with this task might be to copy-and-paste, then
slightly modify the code each time. Or, if you have heard of loops or
used them in other programming languages, you might think to create a
loop. Thankfully R uses something more powerful and expressive than
either of those approaches, and that is functional programming. Using
functional programming concepts, we can use little code to express
complex actions and run large numbers of statistical analyses. This
session will be about using functional programming in the context of
statistical analysis.


split (only works with small to medium sized data, e.g. when your data
can't fit into your computers memory),

## Learning objectives

{{< include /includes/objectives/_stats-analyses-multiple.qmd >}}

## :book: Reading task: Apply logistic regression to each metabolite with functional programming

**Time: \~10 minutes.**

Functional programming underlies many core features of running
statistical methods on data. Because it is such an important component
of this session, you'll briefly review the concepts of functional
programming by going to the [Function
Programming](https://r-cubed-intermediate.rostools.org/sessions/functionals#functional-programming)
in the Intermediate R workshop (**only the reading task section**) as
well as the
[split-apply-combine](https://r-cubed-intermediate.rostools.org/sessions/functionals#split-apply-combine-technique-and-functionals)
(this is a short section).

{{< text_snippet sticky_up >}}

## Using functional programming to run multiple models

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Reinforce the concept of functional programming by briefly going over
the
[figure](https://r-cubed-intermediate.rostools.org/sessions/functionals#fig-functionals)
that visualizes functionals like `map()`.
:::

There are many ways that you can run a model on each metabolite based on
the `lipidomics` dataset. However, these types of
"split-apply-combine" tasks are (usually) best done using data in the
long form. So we'll start with the original `lipidomics` dataset. Create
a header and code chunk at the end of the `docs/learning.qmd` file:

```` {.markdown filename="docs/learning.qmd"}
## Running multiple models

```{{r}}

```
````

The next step is to split the data up. While `group_by()` *might* work, it isn't powerful
enough for what we want to do.
order to make the most use of `{purrr}` functions like `map()`, we will
use `group_split()` to convert the data frame into a set of lists. Let's
first add `{purrr}` as a dependency:

::: callout-note
There is probably a more computationally efficient way of coding this
instead of making a list, but as the saying goes ["premature
optimization is the root of all
evil"](https://stackify.com/premature-optimization-evil/). For our
purposes, this is a very good approach, but for very large datasets and
hundreds of potential models to run, this method would need to be
optimized some more.
:::

```{r purrr-to-deps}
#| purl: true
#| eval: false
#| filename: Console
use_package("purrr")
```

Then we run `group_split()` on the `metabolite` column, which will
output a lot of data frames as a list. The website only shows the first
three.

```{r chain-split-by-metabolite}
#| eval: false
#| filename: "docs/learning.qmd"
lipidomics |>
  group_split(metabolite)
```

```{r}
#| echo: false
lipidomics |>
  group_split(metabolite) |>
  head(3)
```

Remember that logistic regression models need each row to be a single
person

```{r chain-map-to-wider}
#| eval: false
#| filename: "docs/learning.qmd"
lipidomics |>
  group_split(metabolite)
```

```{r}
#| echo: false
fit_all_models <- function(data) {
  list(
    class ~ value,
    class ~ value + age + gender
  ) |>
    map(\(model) fit_model(data, model = model))
}

lipidomics |>
  filter(metabolite == "Cholesterol") |>
  fit_all_models()


lipidomics |>
  group_split(metabolite)

lipidomics |>
  group_split(metabolite) |>
  map(\(data) fit_all_models(data, models)) |>
  head(3)
```

Alright, we now a list of data frames where each data frame has only one
of the metabolites. These bits of code represent the conceptual action
of "splitting the data into a list by metabolites". Since we're
following a function-oriented workflow, let's create a function for
this. Convert it into a function, add the Roxygen documentation using
{{< var keybind.roxygen >}} style using {{< var keybind.styler >}}, move
into the `R/functions.R` file, and then `source()` the file with
{{< var keybind.source >}}.


Like we did with the `metabolite_to_wider()`, we need to pipe the output
into another `map()` function that has a custom function running the
models. We don't have this function yet, so we need to create it. Let's
convert the modeling code we used in the code at the end of section
@sec-fitting-model into a function, replacing `lipidomics` with `data`
and using `starts_with("metabolite_")` within the
`create_recipe_spec()`. Add the Roxygen documentation using
{{< var keybind.roxygen >}}, use {{< var keybind.styler >}} to style,
move into the `R/functions.R` file, and then `source()` the file with
{{< var keybind.source >}}.


Then we add it to the end of the pipe, but using `map()` and
`list_rbind()` to convert to a data frame:

```{r chain-generate-model-results}
#| filename: "docs/learning.qmd"
lipidomics |>
  group() |>
  map(generate_model_results) |>
  list_rbind()
```


## :technologist: Exercise: Update the function to calculate the model estimates

**Time: \~10 minutes.**

Convert the code into a function that calculates the model estimates.
Use this scaffold to help complete the tasks below for the
`calculate_estimates()` function:

``` r
calculate_estimates <- function(data) {
  ___ |>
    # Code from right before the exercise that creates the results
    ___
}
```

1.  Name the new function `calculate_estimates`.
2.  Within the `function()`, add one argument called `data`.
3.  Paste the code we created from above into the function, replacing
    `lipidomics` with `data`.
4.  Add `dplyr::`, `purrr::`, and `stringr::` before their respective
    functions.
5.  Add the Roxygen documentation using {{< var keybind.roxygen >}}.
6.  Use {{< var keybind.styler >}} to style the file to fix up the code.
7.  Cut and paste the function over into the `R/functions.R` file.
8.  Commit the changes you've made so far with {{< var keybind.git >}}.

```{r solution-new-function-calculate-estimates}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
#' Calculate the estimates for the model for each metabolite.
#'
#' @param data The lipidomics dataset.
#'
#' @return A data frame.
#'
calculate_estimates <- function(data) {
  data |>
    split_by_metabolite() |>
    purrr::map(generate_model_results) |>
    purrr::list_rbind() |>
    dplyr::filter(stringr::str_detect(term, "metabolite_"))
}
```

{{< text_snippet sticky_up >}}



## Visualizing the model estimates

We've got one target done for the modeling stage, three more to go!
There are multiple ways of visualizing the results from models. A common
approach is to use a "dot-and-whisker" plot like you might see in a
meta-analysis. Often the "whisker" part is the measure of uncertainty
like the confidence interval, and the "dot" is the estimate. For the
confidence interval, we haven't calculated them at this point because
the typical approach doesn't exactly work for our data (tested before
the workshop). For this plot though, we will use the standard error of
the estimate.

Inside the `docs/learning.qmd`, let's create a new header and code chunk
inside the `## Results` section. We'll want to use
`tar_read(df_model_estimates)` so that `{targets}` is aware that the R
Markdown file is dependent on this target.

```` {.markdown filename="docs/learning.qmd"}
### Figure of model estimates

```{{r}}
model_estimates <- tar_read(df_model_estimates)
```
````

```{r purl-only-model-estimate-md-text}
#| eval: false
#| echo: false
#| purl: true
model_estimate_md_text <- c(
  "### Figure of model estimates",
  "",
  "```{r}",
  "model_estimates <- tar_read(df_model_estimates)",
  "```"
)

print_file("docs/learning.qmd")
revise_by_line_num(
  "docs/learning.qmd",
  model_estimate_md_text,
  # Update numbers
  remove_original_lines = -40,
  insert_at_line = 39
)
git_ci("docs/learning.qmd", "Add code for model estimates to report.")
```

```{r}
#| include: false
model_estimates <- here::here("data/model-estimates.csv") |>
  readr::read_csv(show_col_types = FALSE)
```

Then we'll start using `{ggplot2}` to visualize the results. For
dot-whisker plots, the "geom" we would use is called
`geom_pointrange()`. It requires four values:

-   `x`: This will be the "dot", representing the `estimate` column.
-   `y`: This is the categorical variable that the "dot" is associated
    with, in this case, it is the `metabolite` column.
-   `xmin`: This is the lower end of the "whisker". Since the
    `std.error` is a value representing uncertainty of the estimate on
    either side of it (`+` or `-`), we will need to subtract `std.error`
    from the `estimate`.
-   `xmax`: This is the upper end of the "whisker". Like `xmin` above,
    but adding `std.error` instead.

```{r plot-estimates-pointrange-only}
#| filename: "docs/learning.qmd"
plot_estimates <- model_estimates |>
  ggplot(aes(
    x = estimate,
    y = metabolite,
    xmin = estimate - std.error,
    xmax = estimate + std.error
  )) +
  geom_pointrange()
plot_estimates
```

Woah, there is definitely something wrong here. The values of the
estimate should realistically be somewhere between 0 (can't have a
negative odds) and 2 (in biology and health research, odds ratios are
rarely above 2), definitely unlikely to be more than 5. We will
eventually need to troubleshoot this issue, but for now, let's restrict
the x axis to be between 0 and 5.

```{r plot-estimates-coord-fixed}
#| filename: "docs/learning.qmd"
plot_estimates +
  coord_fixed(xlim = c(0, 5))
```

There are so many things we could start investigating based on these
results in order to fix them up. But for now, this will do.

## :technologist: Exercise: Add plot function as a target in the pipeline

**Time: \~15 minutes.**

Hopefully you've gotten comfortable with the function-oriented workflow,
because we'll need to convert this plot code into a function and add it
as a target in the pipeline. Use the scaffold below as a guide.

``` r
plot_estimates <- function(results) {
  ___ |>
    # Plot code here:
    ___
}
```

1.  Replace `model_estimates` with `results`.
2.  Move the function into the `R/functions.R` file, add the Roxygen
    documentation using {{< var keybind.roxygen >}}, and use
    {{< var keybind.styler >}} to style.

```{r solution-new-function-plot-estimates}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
#' Plot the estimates and standard errors of the model results.
#'
#' @param results The model estimate results.
#'
#' @return A ggplot2 figure.
#'
plot_estimates <- function(results) {
  results |>
    ggplot2::ggplot(ggplot2::aes(
      x = estimate, y = metabolite,
      xmin = estimate - std.error,
      xmax = estimate + std.error
    )) +
    ggplot2::geom_pointrange() +
    ggplot2::coord_fixed(xlim = c(0, 5))
}
```

Then, after doing that, add the new function as a target in the
pipeline, name the new `name` as `fig_model_estimates`. Inside the
`plot_estimates()` function, use the the model estimate target we
created previously (`df_model_estimates`).

``` r
list(
  ...,
  tar_target(
    name = ___,
    command = plot_estimates(___)
  )
)
```

```{r solution-target-plot-estimates}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
list(
  # ...,
  tar_target(
    name = fig_model_estimates,
    command = plot_estimates(df_model_estimates)
  )
)
```

```{r purl-only-tar-make-fig-estimates}
#| eval: false
#| echo: false
#| purl: true
plot_estimates_md_text <- c(
  "",
  "```{{r}}",
  "tar_read(fig_model_estimates)",
  "```",
  ""
)

print_file("docs/learning.qmd")
revise_by_line_num(
  "docs/learning.qmd",
  plot_estimates_md_text,
  # Update numbers
  remove_original_lines = -40,
  insert_at_line = 39
)
git_ci("docs/learning.qmd", "Add code for model estimates to report.")

update_target_plot_est <- "),
tar_target(
  name = fig_model_estimates,
  command = plot_estimates(df_model_estimates)
)
)
"
# print_lines("_targets.R")
styler::style_file("_targets.R")
revise_by_line_num(
  path = "_targets.R",
  insert_text = update_target_calc_est,
  # Modify these numbers.
  remove_original_lines = -30,
  insert_at_line = 31
)
git_ci("_targets", "Add plot estimates target")
```

{{< text_snippet sticky_up >}}

## :book: Reading task: Understanding the model output

**Time: \~10 minutes.**

Let's explain this output a bit, each column at a time:

-   `term`: If you recall the formula
    $class = metabolite + age + gender$, you'll see all but the `class`
    object there in the column `term`. This column contains all the
    predictor variables, including the intercept (from the original
    model).

-   `estimate`: This column is the "coefficient" linked to the term in
    the model. The final mathematical model here looks like:

    $$ \displaylines{class = Intercept + (metabolite\_estimate \times metabolite\_value) + \\ (gender\_estimate \times gender\_value) + ...}$$

    In our example, we chose to get the odds ratios. In the mathematical
    model above, the estimate is represented as the log odds ratio or
    beta coefficient - the constant value you multiply the value of the
    term with. Interpreting each of these values can be quite tricky and
    can take a surprising amount of time to conceptually break down, so
    we won't do that here, since this isn't a statistics workshop. The
    only thing you need to understand here is that the `estimate` is the
    value that tells us the *magnitude* of association between the term
    and `class`. This value, along with the `std.error` are the most
    important values we can get from the model and we will be using them
    when presenting the results.

-   `std.error`: This is the uncertainty in the `estimate` value. A
    higher value means there is less certainty in the value of the
    `estimate`.

-   `statistic`: This value is used to, essentially, calculate the
    `p.value`.

-   `p.value`: This is the infamous value we researchers go crazy for
    and think nothing else of. While there is a lot of attention to this
    single value, we tend to give it more attention than warranted. The
    interpretation of the p-value is even more difficult than the
    `estimate` and again, we won't cover this in this workshop. We won't
    be using this value at all in presenting the results.

{{< text_snippet sticky_up >}}



## Summary

-   Use functional programming with `map()`, as part of the
    function-oriented workflow, to run multiple models efficiently and
    with minimal code.
-   Consistently create small functions that do a specific conceptual
    action and chain them together into larger conceptual actions, which
    can then more easily be incorporated into a `{targets}` pipeline.
    Small, multiple functions combined together are easier to manage
    than fewer, bigger ones.
-   Use dot-whisker plots like `geom_pointrange()` to visualize the
    estimates and their standard error.

{{< include /includes/_code-appendix.qmd >}}
