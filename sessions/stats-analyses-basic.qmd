# A general approach to doing statistical analyses {#sec-stats-analyses-basic}

```{r setup}
#| include: false
source(here::here("R/functions.R"))
extract_functions_from_qmd()
source(here::here("R/project-functions.R"))
library(tidyverse)
library(tidymodels)
lipidomics <- read_csv(here::here("data/lipidomics.csv"))
```

When working with data in research it is almost impossible to avoid
needing to do some form of statistical analysis. Running statistical
analyses is usually methodical and well-defined, though it often
involves trial and error. As it involves several steps: choosing the
right analysis to do for your data, ensuring that your data is
transformed in the right way to do your chosen statistical analysis, and
lastly, extracting the results of your statistical analysis so you can
present them. Sometimes it may feel overwhelming and complicated, which
it definitely can be, but it doesn’t have to be. By taking a structured
approach, you can make the process easier and feel more in control.

In R, statistical methods are often developed by researchers with little
software training, which can make learning new methods challenging.
However, having a framework for running analyses can simplify the
process.

For this session, we are going to learn how we can generally set up and
run statistical models, so that we can use it within `{targets}`.

## Learning objectives {#sec-stats-analyses-basic-learning-objectives}

This session’s **overall learning outcome** is to:

1.  Describe the basic framework underlying most statistical analyses
    and use R to generate statistical results using this framework.

**Specific objectives** are to:

1.  Describe the general "workflow": State the research question,
    construct a model, prepare the data, fit the model, and extract the
    results.
2.  Categorize the model definition step as a distinct, theory-driven
    step, separate from the data, and use `{parsnip}` functions to help
    with defining your model.
3.  Identify some data transformation techniques and evaluate which are
    good options given the data. Use functions in the `{recipes}`
    package to apply these transformations to your data.
4.  Use the `{broom}` package to extract the model results to later
    present them in graphical format (with `{ggplot2}`).
5.  Continue applying the concepts and functions used from the previous
    sessions.

Specific "anti"-objectives on things that we will *not* learn:

-   How to choose and apply the appropriate statistical model or tests.
-   Statistical theory.
-   How to determine relevant data transformations for statistical
    tests.
-   How to interpret the statistical results.

This means that what we show is *for demonstration purposes only*. Our
implementation and choice of model and analyses could be wrong if an
expert were to review them.

::: {.callout-warning appearance="default"}
We will be making *a lot* of function throughout this session and the
next. This is just a fair warning!
:::

## :speech_balloon: Discussion activity: What does a "model" mean?

> Time: \~6 minutes.

In science and especially statistics, we talk a lot about "models". But
what does model actually mean? What different types of definitions can
you think of? Is there a different understanding of model in statistics
compared to other areas?

1.  Take 1 minute to think about your understanding of a model.
2.  Then, over the next 3 minutes, discuss with your neighbour about the
    meaning of "model" and see if you can come to a shared
    understanding.
3.  Finally, over the next 2 minutes, we will share all together what a
    model is in the context of data analysis.

## A brief introduction to statistical modelling and workflow

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Let them read it over than go over it briefly, focusing on what a model
is, that we should create research questions while thinking in the
framework of models, and the general workflow for doing statistical
analyses.
:::

<!-- TODO: Create a revealjs presentation on this? -->

:::: callout-note
## :book: Reading task: \~15 minutes

Research, especially quantitative research, is about making inferences
about the world by quantifying uncertainty from some data. And we
quantify uncertainty by using statistics and statistical models.

A statistical model is a simple way to describe the real world using
mathematics. In research, we often aim to understand the relationships
between multiple variables. For example, if we change the predictor
(independent variable or $x$), how does that affect the outcome
(dependent variable or $y$)?

Some simple examples are:

-   "How does this new cancer treatment affect survival rates?"
-   "How does this new user interface affect user watch time?"

or more complex:

-   "How does sunlight and water affect plant growth?"
-   "What is the relationship between a metabolite and type 1 diabetes
    compared to controls, adjusting for age and gender?"

These relationships can involve single or multiple predictors (such as
sunlight and water, or metabolite, gender, and age). The outcomes can be
either continuous (such as plant growth) or categorical (such as whether
a person has type 1 diabetes or not).

The relationship between predictors and the outcome is known as a
regression. If there is a relationship between ($x$) and ($y$), the
corresponding mathematical object is a function, $f()$:

$$y = f(x)$$

Since no relationship is perfect, we add some random error (error) to
describe the difference between the function and the actual measured
variables:

$$y = f(x) + error$$

The simplest form of regression is linear regression, where the formula
is a linear combination:

$$y = intercept + x + error$$

Here, the $intercept$ is the value of $y$ when $x$ is zero.

For example, we might expect a relationship between plant growth, water,
and sunlight. We could model plant growth as depending on water and
sunlight. Graphically, we can illustrate this model as:

```{mermaid fig-model-plant-growth}
%%| label: fig-model-plant-growth
%%| fig-cap: Simple example of a theoretical model of plant growth.
%%| echo: false
%%| eval: true
graph LR
    Sunlight --> Growth
    Water --> Growth
    Sunlight --- Water
```

or mathematically:

$$growth = sunlight + water$$

This expression is known as our theoretical model. However, not all
theoretical models can be tested against the real world. We need to
structure the theoretical model so that we can measure the parameters---
the variables like growth, sunlight, and water in the model above---
involved. In this case, we need to measure plant growth (e.g., weight in
grams), the amount of water given per day (in liters), and the number of
sunlight hours per day.

This also means that when formulating a research question, there is an
order, or workflow, to follow:

1.  **Write a research question** that fits a theoretical model with
    measurable parameters.

2.  **Select the best model type** based on the parameters (e.g., linear
    regression, logistic regression, ANOVA, or t-test).

3.  **Measure your parameters** (e.g., water in liters per day, plant
    growth in weight, sunlight in hours per day).

4.  **Fit the data to the model** to estimate the values (coefficients)
    of the model, such as the intercept, slope, and uncertainty (error).

5.  **Extract and present the values** in relation to your research
    questions.

```{mermaid fig-model-building-workflow}
%%| label: fig-model-building-workflow
%%| fig-cap: Simple schematic of the workflow for conducting statistical analysis.
%%| echo: false
%%| eval: true
graph TD
    A[Research question] --> B((Statistical model))
    B --> A
    B --> C[Data collection]
    C --> D[Data transformation]
    D --> E[Scientific output]
    E --> A
```

::: {.callout-caution appearance="default"}
The entire workflow for building statistical models requires highly
specific domain knowledge on not only the statistics themselves, but
also how the data was collected, what the values mean, what type of
research questions to ask and how to ask them, how to interpret the
results from the models, and how to process the data to fit the question
and model.

For instance, in our `lipidomics` dataset, if we were to actually use
this data, we would need someone familiar with -omic technologies, how
the data are measured, what the values actually mean, how to prepare
them for the modeling, the specific modeling methods used for this
field, and how we would actually interpret the findings. We have
**none** of these things, so very likely we are doing things quite wrong
here. We're only doing this modeling to highlight how to use the R
packages.
:::

Furthermore, because we are working within a "reproducible analysis"
framework specifically with the use of `{targets}`, we will also convert
questions regarding our lipidomics data into outputs to include as
pipeline targets, along with a basic idea for the final functions that
will make up these targets and their inputs and outputs. These targets
will probably be quite different by the end, but it's a good start to
think about what it should look like in the end.

-   All results for estimated relationships (in case we want to use it
    for other output)
-   Plot of statistical estimate for each relationship

Potential function names might be:

-   `calculate_estimates()`
-   `plot_estimates()`

```{mermaid fig-model-possible-targets}
%%| label: fig-model-possible-targets
%%| fig-cap: Potential inputs, outputs, and functions for the targets pipeline.
%%| echo: false
%%| eval: true
graph TB
    lipidomics -- "calculate_estimates()" --> model_est[Model estimates]
    model_est -- "plot_estimates()" --> plot_est[Plot of estimates]
    plot_est -- "tar_read()" --> qmd[Quarto]
```

{{< include /includes/_sticky-up.qmd >}}
::::

::: {.callout-note collapse="true"}
## :teacher: Instructor note

A few things to repeat and reinforce:

1.  The workflow of the image and that it all starts with the research
    question.
2.  The fact that almost all statistical methods are basically special
    forms of linear regression.
3.  That this model creation stage requires a variety of domain
    expertise, not just statistical expertise.

Also repeat the question to ask, the outputs, as well as the functions
and their flow that we can translate into the `{targets}` pipeline.
:::

## Defining a model for our lipidomics data

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Let them read it first and then briefly verbally walk through this
section, describing the theoretical model both graphically and
mathematically. Go through why we use `{tidymodels}` rather than other
approaches.
:::

::: callout-note
## :book: Reading task: \~10 minutes

Going back to our own lipidomics dataset, we need to do the first step:
Creating the question. While we don’t have much data, there are a
surprising number of questions we could ask. But we will keep it very
simple, very basic, and very exploratory.

1.  What is the estimated relationship of each metabolite with type 1 diabetes
    (T1D) compared to the controls, adjusting for the influence of age and gender?

A graphical representation of this theoretical model could be:

```{mermaid fig-model-research-question}
%%| label: fig-model-research-question
%%| fig-cap: A simple theoretical model of the research question about T1D status.
%%| echo: false
%%| eval: true
graph TB
    Metabolite --> T1D
    Age & Gender --> T1D & Metabolite
```

Or mathematically:

$$T1D = metabolite + age + gender$$

So, T1D status (or `class` in the `lipidomics` dataset) is our
**outcome** and the individual metabolite, age, and gender are our
**predictors**. Technically, age and gender would be "confounders" or
"covariates", since we include them only because we think they influence
the relationship between the metabolite and T1D.

If we convert the formula into a form with the variables we have in the
dataset as well as selecting only one metabolite for now (the
cholesterol metabolite, which we add "metabolite" to differentiate it
from other potential variables), it would be:

$$class = metabolite\_cholesterol + age + gender$$

Now that we have a theoretical model, we need to choose our model type.
Since T1D is binary (either you have it or you don't), the most likely
choice is logistic regression, which requires a binary outcome variable.
So we have the theoretical model and the type of model to use, now how
do we express this as code in R? There are many ways of doing the same
thing in R, but some are a bit easier than others.

One approach that is quite generic and fits with the ideals of the
`{tidyverse}`, is a similar universe of packages called the
`{tidymodels}`. We teach `{tidymodels}` because they are created by
experienced software developers at Posit, the same company behind the
`{tidyverse}` and RStudio. They are known for excellent documentation.
`{tidymodels}` makes it easy to create and use models in a flexible way,
allowing you to switch model types or data processing methods without
needing to learn new tools. This makes your work more efficient and
adaptable.

`{tidymodels}` is designed to clearly separate the different parts of
the model-building process. Each component of the workflow has its own
specific package, making it easier to manage and understand. This
modular approach allows you to change one part of the process, like the
model type or data processing method, without having to learn a
completely new set of tools. This design makes `{tidymodels}` both
flexible and user-friendly.

| Package | Description |
|----|----|
| `{parsnip}` | Define the model, such as type (e.g. `linear_reg()`) and "engine" (e.g. `glm()`). |
| `{recipes}` | Model-specific data transformations, such as removing missing values, or standardizing the data. |
| `{workflows}` | Combining model definition, data, and transformations to calculate the estimates and uncertainty. |

: Core packages within `{tidymodels}`.

We'll start with the `{parsnip}` package. Functions in this package are
used to set the details of the model you want to use. Specifically,
[functions](https://parsnip.tidymodels.org/reference/index.html#models)
to indicate the model *"type"* (e.g. linear regression) and the
`set_engines()` function to determine the "engine" to run the type
(which R-based algorithm to use, like `glm()` compared to `lm()`). Check
out the
[Examples](https://parsnip.tidymodels.org/articles/Examples.html) page
for code you might use depending on the model you want. The most
commonly used model types would be `linear_reg()`, `logistic_reg()`, and
`multinom_reg()`.

{{< include /includes/_sticky-up.qmd >}}
:::

Let's switch to working in the `doc/learning.qmd` file to create those
logistic regression models. First, we need to add `library(tidymodels)`
to the `setup` code chunk. Copy and paste this code chunk below into the
Quarto file.

```` {.markdown filename="doc/learning.qmd"}
```{{r setup}}
targets::tar_config_set(store = here::here("_targets"))
library(tidyverse)
library(targets)
library(tidymodels)
source(here::here("R/functions.R"))
lipidomics <- tar_read(lipidomics)
```
````

Since we will be using `{tidymodels}`, we need to install it, as well as
explicitly add the `{parsnip}`, `{recipes}`, and `{workflows}` packages.
Like `{tidyverse}`, we need to set `{tidymodels}` differently because it
is a "meta-package". We might need to force installing it with
`pak::pak("tidymodels")`.

```{r tidymodels-to-deps}
#| filename: Console
#| purl: true
#| eval: false
use_package("tidymodels", "depends")
# pak::pak("tidymodels")
use_package("parsnip")
use_package("recipes")
use_package("workflows")
```

Before continuing, let's **commit** the changes to the Git history with
{{< var keybind.git >}}. Next, in the `doc/learning.qmd` file, on the
bottom of the document create a new header and code chunk:

```` {.markdown filename="doc/learning.qmd"}
## Building the model

```{{r}}

```
````

In the new code chunk, we will set up the model specs:

```{r logistic-reg-specs}
#| filename: "doc/learning.qmd"
log_reg_specs <- logistic_reg() |>
  set_engine("glm")
log_reg_specs
```

Running this on it's own doesn't show much, as you can see. But we've
now set the model we want to use.

## Data transformations specific to modeling

Setting the model type was pretty easy right? The more difficult part
comes next with the data transformations. `{recipes}` functions are
almost entirely used to apply transformations that a model might
specifically need, like mean-centering, removing missing values, and
other aspects of data processing.

Let's consider our `lipidomics` dataset. In order for us to start our
statistical analysis, we need the data to be structured in a certain way
to be able to smoothly use it as input in our model. We have at least
three necessary transformations, two of which can be done using a single
`{tidyr}` function, while the third one can be fixed with `{recipes}`.
Can you spot them?

```{r}
#| filename: Console
lipidomics
```

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Ask them if they can spot the structural data issues. Give them a few
minutes to think and respond.
:::

The first issue, which isn't always an issue, depends heavily on the
model type you use. Since we are using logistic regression, the model
assumes that each row is an individual person. But our data is in the
long format, so each person has multiple rows. The second issue is that
there seems to be a data entry error, since there are multiple
`Cholesterol` values, while all other metabolites only have one:

```{r too-many-cholesterols}
#| filename: "doc/learning.qmd"
lipidomics |>
  count(code, metabolite) |>
  filter(n > 1)
```

We can fix both the long format and multiple cholesterol issues by using
`tidyr::pivot_wider()`. Before we do, the last issue is that each
metabolite has quite large differences in the values and ranges of data.
Again, whether this is an issue depends on what we want to do, but in
our research question we want to know how each metabolite influences
T1D. In order to best interpret the results and compare across
metabolites, we should ideally have all the metabolites with a similar
range and distribution of values.

Let's fix the first two issues first, and at the same time tidy up the
names of each metabolite so they make better column names. In the
`doc/learning.qmd`, we can tidy up the metabolite names using `mutate()`
with `snakecase::to_snake_case()`. Secondly, to make our dataset wider,
we use `pivot_wider()`. Since we also want an easy way of identifying
columns that are metabolites, we will add a `"metabolite_"` prefix using
the argument `names_prefix`. To actually fix the multiple cholesterol
issue, we should look more into the data documentation or contact the
authors. But for this course, we will merge the values by calculating a
mean before pivoting. We do this by setting the `values_fn` with `mean`.

```{r lipidomic-to-wider}
#| filename: "doc/learning.qmd"
lipidomics_wide <- lipidomics |>
  mutate(metabolite = snakecase::to_snake_case(metabolite)) |>
  pivot_wider(
    names_from = metabolite,
    values_from = value,
    values_fn = mean,
    names_prefix = "metabolite_"
  )
lipidomics_wide
```

Since we're using a function-oriented workflow and since we will be
using this code again later on, let's convert both the "metabolite to
snakecase" and "pivot to wider" code into their own functions. Lets
start writing the function in the `doc/learning.qmd` (we will later move
them over into the `R/functions.R` file).

```{r first-snakecase-fn}
#| filename: "doc/learning.qmd"
column_values_to_snake_case <- function(data) {
  data |>
    dplyr::mutate(metabolite = snakecase::to_snake_case(metabolite))
}
lipidomics |>
  column_values_to_snake_case()
```

This on its own should work. *However*, the column we want to change
might not always be called `metabolite`, or we might want to change it
later. So, to make this function a bit more generic, we can use
something called "curly-curly" (it looks like `{{}}` when used) and
"non-standard evaluation" (NSE).

::: callout-note
## :book: Reading task: \~10 minutes

When you write your own functions that make use of functions in the
`{tidyverse}`, you may eventually encounter an error that might not be
very easy to figure out. Here's a very simple example using `select()`,
where one of your function's arguments is to select columns (just read,
don't write any code):

```{r, error=TRUE}
test_nse <- function(data, columns) {
  data |>
    dplyr::select(columns)
}

lipidomics |>
  test_nse(class)
```

The error occurs because of something called "[non-standard
evaluation](http://adv-r.had.co.nz/Computing-on-the-language.html)" (or
NSE). NSE is a major feature of R and is used quite a lot throughout R.
NSE is used a lot in the `{tidyverse}` packages. It's one of the first
things computer scientists complain about when they use R, because it is
not a common thing in other programming languages. But NSE is what
allows you to use formulas (e.g. `y ~ x + x2` in modeling, which we will
show shortly) or allows you to type out `select(class, age)` or
`library(purrr)`. In "standard evaluation", these would instead be
`select("Gender", "BMI")` or `library("purrr")`. So NSE gives
flexibility and ease of use for the user (we don't have to type quotes
every time) when doing data analysis, but can give some headaches when
programming in R, like when making functions. There's more detail about
this on the [dplyr
website](https://dplyr.tidyverse.org/articles/programming.html#warm-up),
which lists some options to handle NSE while programming. The easiest
approach is to wrap the argument with "curly-curly" (`{{}}`).

```{r}
test_nse <- function(data, columns) {
  data |>
    dplyr::select({{ columns }})
}

lipidomics |>
  test_nse(class)

lipidomics |>
  test_nse(c(class, age))
```

{{< include /includes/_sticky-up.qmd >}}
:::

::: {.callout-note collapse="true"}
## :teacher: Instructor note

You don't need to go over what they read, you can continue with making
the function below. Unless learners have some questions.
:::

We can use curly-curly (combined with `across()`) to apply
`snakecase::to_snake_case()` to columns of our choice.

```{r second-snakecase-fn}
#| filename: "doc/learning.qmd"
column_values_to_snake_case <- function(data, columns) {
  data |>
    dplyr::mutate(dplyr::across({{ columns }}, snakecase::to_snake_case))
}

lipidomics |>
  column_values_to_snake_case(metabolite)
```

Move this new function over into the `R/functions.R` file, add Roxygen
documentation with {{< var keybind.roxygen >}}, style using
{{< var keybind.styler >}}, `source()` the modified `R/functions.R` file
with {{< var keybind.source >}}

```{r new-function-column-values-to-snakecase}
#| filename: "R/functions.R"
#' Convert a column's character values to snakecase format.
#'
#' @param data The lipidomics dataset.
#' @param columns The column you want to convert into the snakecase format.
#'
#' @return A data frame.
#'
column_values_to_snake_case <- function(data, columns) {
  data |>
    dplyr::mutate(dplyr::across({{ columns }}, snakecase::to_snake_case))
}
```

Now, replace the `mutate()` call with our new function (above the
`pivot_wider()` code) in the `doc/learning.qmd` file.

```{r snakecase-function-before-pivot-wider}
#| filename: "doc/learning.qmd"
lipidomics_wide <- lipidomics |>
  column_values_to_snake_case(metabolite) |>
  pivot_wider(
    names_from = metabolite,
    values_from = value,
    values_fn = mean,
    names_prefix = "metabolite_"
  )
lipidomics_wide
```

## :technologist: Exercise: Convert the pivot code into a function

> Time: \~10 minutes.

Just like with the `mutate()`, take the `pivot_wider()` code and convert
it into a new function.

1.  Name the new function `metabolites_to_wider`.
2.  Include one argument in the new `function()`: `data`.
3.  Use `data |>` at the beginning, like we did with the
    `column_values_to_snake_case()`.
4.  Use `tidyr::` before the `pivot_wider()` function.
5.  Add the Roxygen documentation with {{< var keybind.roxygen >}}.
6.  Move the function into the `R/functions.R` file.
7.  Replace the code in the `doc/learning.qmd` file to make use of the
    new functions.

```{r solution-new-function-metabolite-to-wider}
#| eval: true
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
#' Convert the metabolite long format into a wider one.
#'
#' @param data The lipidomics dataset.
#'
#' @return A wide data frame.
#'
metabolites_to_wider <- function(data) {
  data |>
    tidyr::pivot_wider(
      names_from = metabolite,
      values_from = value,
      values_fn = mean,
      names_prefix = "metabolite_"
    )
}
```

{{< include /includes/_sticky-up.qmd >}}

## Using recipes to manage transformations

<!-- TODO: convert this next few paragraphs to reading exercise? -->

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Verbally talk through the next few paragraphs, no code-along yet.
:::

We've used `{dplyr}` and `{tidyr}` to start fixing some of the issues
with the data. But we still have the third issue: How to make the
results between metabolites comparable. That's where we use `{recipes}`.

The first function is `recipe()` and it takes two forms: with or without
a formula. Remember the model formula we mentioned previously? Well,
here is where we can use it to tell `{recipes}` about the model formula
we intend to use so it knows on what variables to apply the chosen
transformations.

For a few reasons that will be clearer later, we won't ultimately use
the formula form of `recipe()`, but will show how it works:

```{r recipes-with-formula}
recipe(class ~ metabolite_cholesterol + age + gender, data = lipidomics_wide)
```

The alternative approach is to set "roles" using `update_roles()`.
Instead of using a formula and letting `recipe()` infer the outcome and
predictors, we can explicitly select which variables are which. This has
some nice features that we will use later on.

```{r recipes-without-formula}
#| filename: "doc/learning.qmd"
#| warning: true
#| message: true
recipe(lipidomics_wide) |>
  update_role(metabolite_cholesterol, age, gender, new_role = "predictor") |>
  update_role(class, new_role = "outcome")
```

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Describe this next paragraph by switching to this website and showing
the list of some of the steps available in the code chunk below.
:::

The next "step" is to select a transformation function. There are many
`step_*` functions (some shown below) that are available in `{recipes}`
and which one you decide to use depends heavily on your data and your
research question. So take your time thinking through which
transformations you actually need for your analysis, rather than quickly
using one or more of them.

```{r}
#| eval: false
recipes::step_log()
recipes::step_scale()
recipes::step_normalize()
recipes::step_center()
recipes::step_sqrt()
```

::: {.callout-tip appearance="default"}
There are so many useful transformation functions available. For
instance, if you often have to impute data, there are functions for
that. You can check them out in the Console by typing
`recipes::step_impute_` then hit the Tab key to see a list of them. Or,
if you have some missing values, there's also the
`recipes::step_naomit()`.
:::

There are many transformations we could use for the `lipidomics`
dataset, but we will use `step_normalize()` for this course. This
function is useful because it makes each variable centered to zero and a
value of 1 unit is translated to 1 standard deviation of the original
distribution. This means we can more easily compare values between
variables. We can add this to the end of the recipe:

```{r recipes-with-step-normalize}
#| filename: "doc/learning.qmd"
#| warning: true
#| message: true
recipe(lipidomics_wide) |>
  update_role(metabolite_cholesterol, age, gender, new_role = "predictor") |>
  update_role(class, new_role = "outcome") |>
  step_normalize(starts_with("metabolite_"))
```

Next thing to do is convert this into a function, using the same
workflow we've been using (which means this needs to be in the
`R/functions.R` script). We'll also use the curly-curly again, since we
might use a different metabolite later. Note, when adding all the
`packagename::` to each function, the `starts_with()` function comes
from the `{tidyselect}` package.

```{r new-function-create-recipe-spec}
#| filename: "R/functions.R"
#' A transformation recipe to pre-process the data.
#'
#' @param data The lipidomics dataset.
#' @param metabolite_variable The column of the metabolite variable.
#'
#' @return
#'
create_recipe_spec <- function(data, metabolite_variable) {
  recipes::recipe(data) |>
    recipes::update_role({{ metabolite_variable }}, age, gender, new_role = "predictor") |>
    recipes::update_role(class, new_role = "outcome") |>
    recipes::step_normalize(tidyselect::starts_with("metabolite_"))
}
```

And test it out:

```{r use-create-recipe-specs-fn}
#| filename: "doc/learning.qmd"
#| warning: true
#| message: true
recipe_specs <- lipidomics_wide |>
  create_recipe_spec(metabolite_cholesterol)
recipe_specs
```

Style using {{< var keybind.styler >}}, then commit the changes made to
the Git history with {{< var keybind.git >}}.

## Fitting the model by combining the recipe, model definition, and data {#sec-fitting-model}

We've now defined the model we want to use and created a transformation
`{recipes}` specification. Now we can start putting them together and
finally fit them to the data. This is done with the `{workflows}`
package.

Why use this package, rather than simply run the statistical analysis
and process the data as normal? When running multiple models (like we
will do in the next section) that may require different data structures,
the data transformation steps have to happen *right before* the data is
fit to the model and need to be done on exactly the data used by the
model. So if we have one data frame that we run multiple models on, but
the transformation happens to the whole data frame, we could end up with
issues due to how the transformations were applied. The `{workflows}`
package keeps track of those things for us, so we can focus on the
higher level thinking rather than on the small details of running the
models.

The `{workflows}` package has a few main functions for combining the
recipe with the model specs, as well as several for updating an existing
workflow (which might be useful if you need to run many models of
slightly different types). All model workflows need to start with
`workflow()`, followed by two main functions: `add_model()` and
`add_recipe()`. Can you guess what they do?

```{r use-workflow-for-model}
#| filename: "doc/learning.qmd"
workflow() |>
  add_model(log_reg_specs) |>
  add_recipe(recipe_specs)
```

While this code is already pretty concise, let's convert it into a
function to make it simplified. We'll use the same function-oriented
workflow that we've used before, where the function should ultimately be
inside the `R/functions.R` file.

```{r new-function-create-model-workflow}
#| filename: "R/functions.R"
#' Create a workflow object of the model and transformations.
#'
#' @param model_specs The model specs
#' @param recipe_specs The recipe specs
#'
#' @return A workflow object
#'
create_model_workflow <- function(model_specs, recipe_specs) {
  workflows::workflow() |>
    workflows::add_model(model_specs) |>
    workflows::add_recipe(recipe_specs)
}
```

<!-- TODO: image showing pieces at play that we need to eventually fit into targets/questions -->

Instead of using the previously created objects, let's start the model
creation from scratch:

```{r full-model-workflow-from-almost-scratch}
#| filename: "doc/learning.qmd"
model_workflow <- create_model_workflow(
  logistic_reg() |>
    set_engine("glm"),
  lipidomics_wide |>
    create_recipe_spec(metabolite_cholesterol)
)
model_workflow
```

Now, we can do the final thing: Fitting the data to the model with
`fit()`! :raised_hands:

```{r fit-model-workflow-to-data}
#| filename: "doc/learning.qmd"
fitted_model <- model_workflow |>
  fit(lipidomics_wide)
fitted_model
```

This gives us a lot of information, but what we are mostly interested in
is the model estimates themselves. While this `fitted_model` object
contains a lot additional information inside, `{workflows}` thankfully
has a function to extract the information we want. In this case, it is
the `extract_fit_parsnip()` function.

```{r extract-model-fit}
#| filename: "doc/learning.qmd"
fitted_model |>
  extract_fit_parsnip()
```

To get this information in a tidier format, we use another function:
`tidy()`. This function comes from the `{broom}` package, which is part
of the `{tidymodels}`. But we should explicitly add it to the
dependencies:

```{r broom-to-deps}
#| purl: true
#| eval: false
#| filename: Console
use_package("broom")
```

Then, we add the `tidy()` function to our model using the `|>` pipe.
Since we are using a logistic regression model, we need to consider how
we want the estimates to be presented, probably depending on how we want
to visualize our results. If we set `exponentiate = TRUE` in `tidy()`,
the output estimates will be odds ratios, if we set
`exponentiate = FALSE`, we will get the log odds ratios. Here we choose
`exponentiate = TRUE`:

```{r tidy-up-model-results}
#| filename: "doc/learning.qmd"
fitted_model |>
  extract_fit_parsnip() |>
  tidy(exponentiate = TRUE)
```

We now have a data frame of our model results!

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Reinforce **not** to worry about interpreting the results, that is not
the aim of this course.
:::

::: callout-important
Another reminder, we **are not** interpreting this results. For this
course, they are **not** important and can distract from the main
purpose.
:::

Like we did with the `workflows()` code that we converted into a
function, we do the same thing here: Make another function (and move it
to `R/functions.R`)! :stuck_out_tongue:

```{r new-function-tidy-model-output}
#| filename: "R/functions.R"
#' Create a tidy output of the model results.
#'
#' @param workflow_fitted_model The model workflow object that has been fitted.
#'
#' @return A data frame.
#'
tidy_model_output <- function(workflow_fitted_model) {
  workflow_fitted_model |>
    workflows::extract_fit_parsnip() |>
    broom::tidy(exponentiate = TRUE)
}
```

Replacing the code in the `doc/learning.qmd` file to use the function.

```{r use-tidy-model-output-fn}
#| filename: "doc/learning.qmd"
fitted_model |>
  tidy_model_output()
```

If we revise the code so it is one pipe, it would look like:

```{r single-pipe-model-results}
#| filename: "doc/learning.qmd"
create_model_workflow(
  logistic_reg() |>
    set_engine("glm"),
  lipidomics_wide |>
    create_recipe_spec(metabolite_cholesterol)
) |>
  fit(lipidomics_wide) |>
  tidy_model_output()
```

Let's briefly cover what these columns and values mean.

::: {.callout-note collapse="true"}
## :teacher: Instructor note

If you want, you can go over these details briefly or in more detail,
depending on how comfortable you are. Or you can get them to read it
only.
:::

::: callout-note
## :book: Reading task: \~10 minutes

Let's explain this output a bit, each column at a time:

-   `term`: If you recall the formula
    $class = metabolite + sex + gender$, you'll see all but the `class`
    object there in the column `term`. This column contains all the
    predictor variables, including the intercept (from the original
    model).

-   `estimate`: This column is the "coefficient" linked to the term in
    the model. The final mathematical model here looks like:

    $$ \displaylines{class = Intercept + (metabolite\_estimate \times metabolite\_value) + \\ (gender\_estimate \times gender\_value) + ...}$$

    In our example, we chose to get the odds ratios. In the mathematical
    model above, the estimate is represented as the log odds ratio or
    beta coefficient - the constant value you multiply the value of the
    term with. Interpreting each of these values can be quite tricky and
    can take a surprising amount of time to conceptually break down, so
    we won't do that here, since this isn't a statistics course. The
    only thing you need to understand here is that the `estimate` is the
    value that tells us the *magnitude* of association between the term
    and `class`. This value, along with the `std.error` are the most
    important values we can get from the model and we will be using them
    when presenting the results.

-   `std.error`: This is the uncertainty in the `estimate` value. A
    higher value means there is less certainty in the value of the
    `estimate`.

-   `statistic`: This value is used to, essentially, calculate the
    `p.value`.

-   `p.value`: This is the infamous value we researchers go crazy for
    and think nothing else of. While there is a lot of attention to this
    single value, we tend to give it more attention than warranted. The
    interpretation of the p-value is even more difficult than the
    `estimate` and again, we won't cover this in this course. We won't
    be using this value at all in presenting the results.

{{< include /includes/_sticky-up.qmd >}}
:::

Before ending, open the Git interface and commit the changes you made
with {{< var keybind.git >}}. Then push your changes up to GitHub.

## Summary

-   Create research questions that (ideally) are structured in a way to
    mimic how the statistical analysis will be done, preferably in a
    "formula" style like $y = x1 + x2 + ... + error$ and in a diagram
    style with links connecting variables.
-   Statistical analyses, while requiring some trial and error, are
    surprisingly structured in the workflow and steps taken. Use this
    structure to help guide you in completing tasks related to running
    analyses.
-   Use `{parsnip}` functions to define the model you want to use, like
    `logistic_reg()` for logistic regression, and set the computational
    "engine" with `set_engine()`.
-   Use `{recipes}` functions to set up the data transformation steps
    necessary to effectively run the statistical analysis, like adding
    variable "roles" (outcome vs predictor) using `update_roles()` and
    adding transformation steps using any of the dozen different `step_`
    functions.
-   Use `{workflows}` functions to develop an analysis `workflow()` that
    combines the defined model with `add_model()`, the transformation
    steps with `add_recipe()`, and the data with `fit()`.
-   Use `{broom}` to `tidy()` the model output, extracted using
    `extract_fit_parsnip()` to get a data frame of the estimates and
    standard error for the variables in the model.

{{< include /includes/_code-appendix.qmd >}}
