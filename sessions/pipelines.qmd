---
execute:
  eval: false
---

# Creating automatic analysis pipelines {#sec-pipelines}

```{r setup}
#| include: false
#| eval: true
source(here::here("R/functions.R"))
library(tidyverse)
library(targets)
lipidomics <- read_csv(here::here("data/lipidomics.csv"))
```

{{< text_snippet review_note >}}

When doing analyses on data, you have probably experienced (many) times
where you forget the order in which other code should run or which parts
need to be rerun to update results. Things can get confusing quickly,
even in relatively simple projects. This becomes even more challenging
when you return to a project after a month or two and have forgotten the
state of the analysis and the project as a whole.

That's where formal data analysis pipeline tools come in. By organising
your analysis into distinct steps, with clear inputs and outputs, and
adding these steps to a pipeline that tracks them, you can make things a
lot easier for yourself and others. This session focuses on using tools
that create and manage these pipelines effectively.

## Learning objectives

{{< include /includes/objectives/_pipelines.qmd >}}

## :speech_balloon: Discussion activity: How do you rerun analyses when something changes?

**Time: ~12 minutes.**

We've all been in situations where something in our analysis needs to
change: Maybe we forgot to remove a certain condition (like unrealistic
BMI), maybe our supervisor suggests something we hadn't considered in
our analysis, or maybe during peer review of our manuscript, a reviewer
makes a suggestion that would improve the understanding of the paper.

Whatever the situation, we inevitably need to rerun parts of or the full
analysis. So what is your **exact** workflow when you need to rerun code
and update your results? Assume it's a change somewhere early in the
data processing stage.

1.  Take about 1 minute to think about the workflow you use. Try to
    think of the *exact* steps you need to take, what *exactly* you do,
    and how long that usually takes.
2.  For 8 minutes, share and discuss your thoughts in your group. How do
    your experiences compare to each other?
3.  For the remaining time, we'll briefly share with everyone what
    they've thought and discussed.

## :book: Reading task: What is a data analysis "pipeline"?

**Time: ~10 minutes.**

::: {.callout-note collapse="true"}
## :teacher: Instructor note

After they finish reading this section, briefly walk through it. In
particular, emphasize what we want to make at the end, even though that
goal might change as the analysis progresses.
:::

A pipeline can be any process where the steps between a start and an end
point are very clear, explicit, and concrete. These highly distinct
steps can be manual, human involved, or completely automated by a robot
or computer. For instance, in car factories, the pipeline from the input
raw materials to the output vehicle is extremely well described and
implemented. Similarly, during the pandemic, the pipeline for testing
(at least in Denmark and several other countries) was highly structured
and clear for both the workers doing the testing and the people having
the test done: A person goes in, scans their ID card, has the test done,
the worker inputs the results, and the results are sent immediately to
the health agency as well as to the person based on their ID contact
information (or via a secure app).

However, in research, especially around data collection and analysis, we
often hear or read about "pipelines". But looking closer, these aren't
actual pipelines because the individual steps are not very clear and not
well described, and they often require a fair amount of manual human
attention and intervention. Particularly within computational
environments, a pipeline should be a set of data processing steps
connected in a series with a specific order; the output of one step is
the input to the next. This means that there actually should be minimal
to *no* human intervention from raw input to finished output.

Why aren't these computational data pipelines found in most of the
"pipelines" described in research? Because:

1.  Anything with data ultimately must be on the computer,
2.  Anything automatically done on the computer must be done with code,
3.  Not all researchers write code,
4.  Researchers who do write code rarely publish and share it,
5.  Code that is shared or published (either publicly or within the
    research group) is not written in a way that allows a pipeline to
    exist,
6.  And, research is largely non-reproducible [@Trisovic2022;
    @Seibold2021; @Laurinavichyute2021].

A data analysis pipeline would, by definition, be a readable and
reproducible data analysis. Unfortunately, we researchers, as a group,
don't make use of the tools to implement actual data analysis pipelines.

This isn't to diminish the work of researchers, but is rather a basic
observation on the systemic, social, and structural environment
surrounding us. We as researchers are not necessarily trained in writing
code, nor do we have a strong culture and incentive structure around
learning, sharing, reviewing, and improving code. We are also very
rarely allowed to get (or use) funds to hire people who *are* trained
and skilled in programmatic thinking and coding. Otherwise workshops like
this wouldn't need to exist :woman_shrugging:

Before we get to what an actual data analysis pipeline looks like in
practice, we have to separate two things: *exploratory data analysis*
and *final paper data analysis*. In exploratory data analysis, there
will likely be a lot of manual, interactive steps involved that may or
may not need to be explicitly stated and included in the analysis plan
and pipeline. But for the final paper and the included results, we
generally have some basic first ideas of what we'll need. Let's list a
few items that we would want to do before the primary statistical
analysis:

1.  A table of some basic descriptive statistics of the study
    population, such as mean, standard deviation, or counts of basic
    discrete data (like treatment group).
2.  A figure showing the distribution of your main variables of
    interest. In this case, ours are the lipidomic variables.
3.  The paper with the results included.

```{mermaid fig-pipeline-schematic}
%%| fig-cap: Simplified flow chart of some of the initial steps in a data analysis pipeline.
%%| echo: false
%%| eval: true
graph LR
    data[Data] --> fn_desc{{function}}
    fn_desc --> tab_desc[Output 1:<br>Descriptive<br>statistics<br>table]
    tab_desc --> paper[Output 3:<br>Paper]
    data --> fn_plot_vars{{function}}
    fn_plot_vars{{function}} --> plot_distrib[Output 2:<br>Continuous<br>variables<br>plot]
    plot_distrib --> paper
```

Now that we have conceptually drawn out some initial tasks in our
pipeline, we can start using R to build it.

{{< text_snippet sticky_up >}}

## Using targets to manage a pipeline

There are a few packages that help build pipelines in R, but the most
commonly used, well-designed, and maintained one is called `{targets}`.
This package allows you to explicitly specify the outputs you want to
create. `{targets}` will then track them for you and know which output
depends on which as well as which ones need to be updated when you make
changes to your pipeline.

::: {.callout-note collapse="true"}
## :teacher: Instructor note

Ask participants, which do they think it is: a build dependency or a
workflow dependency. Because it is directly used to run analyses and
process the data, it would be a build dependency.
:::

First, we need to install and add `{targets}` to our dependencies. Since
`{targets}` is a build dependency, we'll add it to the `DESCRIPTION`
file with:

```{r add-targets-as-dep}
#| purl: true
#| filename: Console
use_package("targets")
```

Now that it's added to the project R library, let's set up our project
to start using it!

```{r use-targets}
#| purl: true
#| filename: Console
targets::use_targets()
```

This command will write a `_targets.R` script file that we'll use to
define our analysis pipeline. Before we do that though, let's **commit
the file to the Git history** with {{< var keybind.git >}}.

::: callout-note
If you are using an older version of the `{targets}` package, you might
also find that running `targets::use_targets()` also have created a
`run.R` and `run.sh` file. The files are used for other situations (like
running on a Linux server) that we wonâ€™t cover in this workshop.
:::

## :book: Reading task: Inside of the `_targets.R` file

**Time: ~8 minutes.**



::: {.callout-note collapse="true"}
## :teacher: Instructor note

Let them read it before going over it again to reinforce
function-oriented workflows and how `{targets}` and the `tar_target()`
works.
:::

For this reading task, we'll take a look at what the `_targets.R`
contains. So, start by opening it on your computer.

::: {.callout-note collapse="true"}
## Click this to see the file contents

```{r}
#| eval: true
#| echo: false
#| code-fold: true
fs::path_package("targets", "pipelines", "use_targets.R") |>
  readLines() |>
  cat(sep = "\n")
```
:::

Mostly, the`_targets.R` script contains comments and a basic set up to
get you started. But, notice the `tar_target()` function used at the end
of the script. There are two main arguments for it: `name` and
`command`. The way that `{targets}` works is similar to how you'd assign
the output of a function to an object, so:

``` r
object_name <- function_in_command(input_arguments)
```

Is the same as:

``` r
tar_target(
  name = object_name,
  command = function_in_command(input_arguments)
)
```

What this means is that `{targets}` follows a ["function-oriented"
workflow](https://books.ropensci.org/targets/functions.html#problems-with-script-based-workflows),
not a "script-based" workflow. What's the difference? In a
script-oriented workflow, each R file/script is run in a specific order.
As a result, you might end up with an R file that has code like:

``` r
source("R/1-process-data.R")
source("R/2-basic-statistics.R")
source("R/3-create-plots.R")
source("R/4-linear-regression.R")
```

While in a function-oriented workflow, it might look more like:

``` r
source("R/functions.R")
raw_data <- load_raw_data("file/path/data.csv")
processed_data <- process_data(raw_data)
basic_stats <- calculate_basic_statistics(processed_data)
simple_plot <- create_plot(processed_data)
model_results <- run_linear_reg(processed_data)
```

With the function-oriented workflow, each function takes an input and
contains all the code to create one result as its output. This could be,
for instance, a figure in a paper.

If you've taken the [intermediate R
workshop](https://r-cubed-intermediate.rostools.org/), you'll notice that
this function-oriented workflow is the workflow we covered in that
workshop. There are so many advantages to this type of workflow which is
why many powerful R packages are designed around making use of this
workflow.

If we take these same code as above and convert it into the `{targets}`
format, the end of `_targets.R` file would like this:

``` r
list(
  tar_target(
    name = raw_data,
    command = load_raw_data("file/path/data.csv")
  ),
  tar_target(
    name = processed_data,
    command = process_data(raw_data)
  ),
  tar_target(
    name = basic_stats,
    command = calculate_basic_statistics(processed_data)
  ),
  tar_target(
    name = simple_plot,
    command = create_plot(processed_data)
  ),
  tar_target(
    name = model_results,
    command = run_linear_reg(processed_data)
  )
)
```

So, each `tar_target` is a step in the pipeline. The `command` is the
function call, while the `name` is the name of the output of the
function call.

{{< text_snippet sticky_up >}}

## Building up our first target

Let's start writing code to create the three items we listed above in
@fig-pipeline-schematic:

1.  Some basic descriptive statistics
2.  A plot of the continuous lipid variables
3.  A report (Quarto document).

Since we'll use `{tidyverse}`, specifically `{dplyr}`, to calculate the
summary statistics, we need to add it to our dependencies. Because
`{tidyverse}` is a "meta"-package (special type of package that makes it
easy to install and load other packages), we need to add it to the
`"depends"` section of the `DESCRIPTION` file.

```{r add-tidyverse-deps}
#| filename: Console
#| purl: true
use_package("tidyverse", "depends")
```

Commit the changes made to the `DESCRIPTION` file in the Git history
with {{< var keybind.git >}}.

Now, let's start creating the code for our outputs, so that we can add
it to our pipeline later. First, open up the `doc/learning.qmd` file and
create a new header and code chunk at the bottom of the file.

```` {.markdown filename="doc/learning.qmd"}

```{{r setup}}
library(tidyverse)
source(here::here("R/functions.R"))
lipidomics <- read_csv(here::here("data/lipidomics.csv"))
```

## Basic statistics

```{{r basic-stats}}

```
````

```{r purl-only-markdown-basic-stats}
#| eval: false
#| echo: false
#| purl: true
git_ci("DESCRIPTION", "Add tidyverse and targets to deps")
c(
  "\n## Basic statistics\n\n```{r setup}\nlibrary(tidyverse)",
  "source(here::here('R/functions.R'))",
  "lipidomics <- read_csv(here::here('data/lipidomics.csv'))\n```\n\n"
) |>
  paste0(collapse = "\n") |>
  write_to_file("doc/learning.qmd")
git_ci("doc/learning.qmd", "Add section on basic stats to qmd")
```

For our first output, we want to calculate the mean and SD for each
metabolite and then, to make it more readable, we want to round the
numbers to one digit. To do this, we will use the split-apply-combine
methodology and the `across()` function, which we covered in the
[functionals](https://r-cubed-intermediate.rostools.org/sessions/functionals)
session of the intermediate workshop.

Briefly, we will:

1.  Use `group_by()` to split the dataset based on the metabolites.
2.  Use `summarise()` to calculate the `mean()` and standard deviation
    (`sd()`) for each metabolite.
3.  Round the calculated mean and standard deviation to 1 digit using
    `round()`.

While doing these steps, we will use the `across()` functional, making
it easy to apply the same transformation on multiple columns.

Let's write out the code!

```{r mean-sd-by-each-metabolite}
#| filename: "doc/learning.qmd"
#| eval: true
# In the {r basic-stats} chunk
lipidomics |>
  group_by(metabolite) |>
  summarise(across(value, list(mean = mean, sd = sd))) |>
  mutate(across(where(is.numeric), ~ round(.x, digits = 1)))
```

After that, style the file using {{< var keybind.styler >}}. Then,
commit the changes to the Git history with {{< var keybind.git >}}.

## :technologist: Exercise: Convert summary statistics code into a function

**Time: ~20 minutes.**

In the `doc/learning.qmd` file, use the "function-oriented" workflow, as
taught in the [intermediate
workshop](https://r-cubed-intermediate.rostools.org/sessions/functions#the-basics-of-a-function),
to take the code we wrote above and convert it into a function.

Complete these tasks:

1.  Convert the code into a function by wrapping it with
    `function() {...}` and name the new function `descriptive_stats`.
    Here is some scaffolding to help you get started:

    ``` {.r filename="doc/learning.qmd"}
    descriptive_stats <- function(___) {
      ___
    }
    ```

2.  Replace `lipidomics` in the code we wrote before with `data` and put
    `data` as an argument inside the brackets of `function()`.

3.  At the start of each function we use inside our `descriptive_stats`
    function, add the name of the package like so: `packagename::`.
    E.g., add `dplyr::` before `group_by()` because it's from
    the`{dplyr}` package. Remember that you can look up which package a
    function comes from by writing `?functionname` in the Console.

4.  Style the code using {{< var keybind.styler >}} to make sure it is
    formatted correctly. You might need to manually force a styling if
    lines are too long.

5.  With the *cursor* inside the function, add some roxygen
    documentation with {{< var keybind.palette >}} followed by typing
    "roxygen comment". Remove the lines that contain `@examples` and
    `@export`, then fill in the other details (like the `@params` and
    `Title`). In the `@return` section, write "A data.frame/tibble."

6.  Cut and paste the function over into the `R/functions.R` file.

7.  Source the `R/functions.R` file with {{< var keybind.source >}}, and
    then test the code by running `descriptive_stats(lipidomics)` in the
    Console. If it works, do the last task.

8.  Save both files (`learning.qmd` and `functions.R`). Then open the
    Git interface and commit the changes you made to them with
    {{< var keybind.git >}}.

::: {.callout-tip appearance="default" title="Tip: Implicit returns"}
In the intermediate workshop, we suggested using `return()` at the end of
the function. Technically, we don't need an explicit `return()`, since
the output of the last code that R runs within the function will be the
output of the function. This is called an *implicit return* and we will
be using this feature throughout the rest of this workshop.
:::

```{r solution-descriptive-stats}
#| eval: true
#| code-fold: true
#| code-summary: "**Click for a potential solution**. Only click if you are struggling or are out of time."
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data The lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data |>
    dplyr::group_by(metabolite) |>
    dplyr::summarise(dplyr::across(value, list(mean = mean, sd = sd))) |>
    dplyr::mutate(dplyr::across(tidyselect::where(is.numeric), ~ round(.x, digits = 1)))
}
```

{{< text_snippet sticky_up >}}

## Adding a step in the pipeline

Now that we've created a function to calculate some basic statistics, we
can now add it as a step in the `{targets}` pipeline. Open up the
`_targets.R` file and go to the end of the file, where the `list()` and
`tar_target()` code are found. In the first `tar_target()`, replace the
target to load the lipidomic data. In the second, replace it with the
`descriptive_stats()` function. If we want to make it easier to remember
what the target output is, we can add `df_` to remind us that it is a
data frame. It should look like:

```{r targets-basic-stats}
#| eval: false
#| filename: "targets.R"
list(
  tar_target(
    name = lipidomics,
    command = readr::read_csv(here::here("data/lipidomics.csv"))
  ),
  tar_target(
    name = df_stats_by_metabolite,
    command = descriptive_stats(lipidomics)
  )
)
```

Let's run `{targets}` to see what happens! You can either use
{{< var keybind.targets-make >}} or run this code in the Console:

```{r targets-make}
#| eval: false
#| filename: "Console"
targets::tar_make()
```

<!-- TODO: Check how this works, and update next section -->

While this `{targets}` pipeline works, it is currently not able to
invalidate the pipeline if our underlying data changes. To track the
actual data file we need to create a separate pipeline target, where we
define our actual file using the argument `format = "file"`. We can then
change our previous target to point to the newly defined file target.
Lets do that now.

```{r targets-basic-stats-by-metabolite}
#| eval: false
#| filename: "targets.R"
list(
  tar_target(
    name = file,
    command = "data/lipidomics.csv",
    format = "file"
  ),
  tar_target(
    name = lipidomics,
    command = readr::read_csv(file, show_col_types = FALSE)
  ),
  tar_target(
    name = df_stats_by_metabolite,
    command = descriptive_stats(lipidomics)
  )
)
```

```{r purl-only-add-to-targets}
#| eval: false
#| echo: false
#| purl: true
new_targets <- '
list(
    tar_target(
        name = file,
        command = "data/lipidomics.csv",
        format = "file"
    ),
    tar_target(
        name = lipidomics,
        command = readr::read_csv(file, show_col_types = FALSE)
    ),
    tar_target(
        name = df_stats_by_metabolite,
        command = descriptive_stats(lipidomics)
    )
)
'
# print_lines("_targets.R")
revise_by_line_num(
  path = "_targets.R",
  insert_text = new_targets,
  remove_original_lines = -28:-38,
  insert_at_line = 28
)
git_ci("_targets.R", "Add new targets to run.")
```

Since we finished writing some code, let's style the file using
{{< var keybind.styler >}}. Now, let's try running `{targets}` again
using {{< var keybind.targets-make >}}. It should run through! We also
see that a new folder has been created called `_targets/`. Inside this
folder it will keep all of the output from running the code. It comes
with i's own `.gitignore` file so that you don't track all the files
inside, since they aren't necessary. Only the `_targets/meta/meta` is
needed to include in Git.

We can visualize our individual pipeline targets that we track through
`tar_target()` now too, which can be useful as you add more and more
targets. We will (likely) need to install an extra package (done
automatically):

```{r visualize-targets}
#| purl: true
#| filename: Console
targets::tar_visnetwork()
```

Or to see what pipeline targets are outdated:

```{r outdated-targets}
#| purl: true
#| filename: Console
targets::tar_outdated()
```

Before continuing, let's commit the changes (including the files in the
`_targets/` folder) to the Git history with {{< var keybind.git >}}.

```{r purl-only-commit-packages-to-targets}
#| eval: false
#| echo: false
#| purl: true
git_ci(
  c("_targets", "_targets.R", "DESCRIPTION"),
  "Tell targets about packages, add _targets"
)
```

## Creating figure outputs

Not only can we create data frames with targets (like above), but also
figures. Let's write some code to create the plot we listed as our
"output 2" in @fig-pipeline-schematic. Since we're using `{ggplot2}` to
write this code, let's add it to our `DESCRIPTION` file.

```{r add-ggplot2-deps}
#| purl: true
#| filename: Console
use_package("ggplot2")
```

Next, we'll switch back to `doc/learning.qmd` and write the code to this
plot of the distribution of each metabolite. We'll use
`geom_histogram()`, nothing too fancy. And since the data is already in
long format, we can easily use `facet_wrap()` to create a plot for each
metabolite. We use `scales = "free"` because each metabolite doesn't
have the same range of values (some are small, others are quite large).

```{r histogram-metabolites}
#| fig-cap: "Histograms showing the distribution of all metabolites in the lipidomics dataset."
#| eval: true
#| filename: "doc/learning.qmd"
metabolite_distribution_plot <- ggplot(lipidomics, aes(x = value)) +
  geom_histogram() +
  facet_wrap(vars(metabolite), scales = "free")
metabolite_distribution_plot
```

We now have the basic code to convert over into functions.

## :technologist: Exercise: Convert the plot code to a function

**Time: ~10 minutes.**

For now, we will only take the code to make the distribution plot and
convert it into a function. Just like you did with the
`descriptive_stats()` function in the exercise above, complete these
tasks:

1.  Wrap the plot code inside `doc/learning.qmd` with `function() {...}`
    and name the new function `plot_distributions`. Use this scaffolding
    code to help guide you to write the code into a function.

    ``` {.r filename="doc/learning.qmd"}
    plot_distributions <- function(___) {
      ___
    }
    ```

2.  Replace `lipidomics` with `data` and put `data` as an argument
    inside the brackets of `function()`.

3.  Add `ggplot2::` to the start of each `{ggplot2}` function used
    inside your function.

4.  Style using {{< var keybind.styler >}} to make sure it is formatted
    correctly. You might need to manually force a styling if lines are
    too long.

5.  With the *cursor* inside the function, add some roxygen
    documentation with {{< var keybind.roxygen >}}. Remove the lines
    that contain `@examples` and `@export`, then fill in the other
    details (like the `@params` and `Title`). In the `@return` section,
    write "A plot object."

6.  Cut and paste the function over into the `R/functions.R` file.

7.  Source the `R/functions.R` file ({{< var keybind.source >}}) and
    then test the code by running `plot_distributions(lipidomics)` in
    the Console. If it works, do the last task.

8.  Save both files and then open the Git interface and commit the
    changes you made to them with {{< var keybind.git >}}.

```{r solution-new-function-descriptive-plots}
#| eval: false
#| code-fold: true
#| code-summary: "**Click for the solution**. Only click if you are struggling or are out of time."
## This should be in the R/functions.R file.
#' Plot for basic distribution of metabolite data.
#'
#' @param data The lipidomics dataset.
#'
#' @return A ggplot2 graph.
#'
plot_distributions <- function(data) {
  data |>
    ggplot2::ggplot(ggplot2::aes(x = value)) +
    ggplot2::geom_histogram() +
    ggplot2::facet_wrap(ggplot2::vars(metabolite), scales = "free")
}
```

::: {.callout-note appearance="minimal" collapse="true" icon="false"}
## :technologist: Extra exercise: Test out how `tar_outdated()` and `tar_visnetwork()` work

**Time: ~10 minutes.**

Let's make a change to our function and test out how the
`tar_outdated()` and `tar_visnetwork()` work.

1.  Open up the `R/functions.R` file and go to the `descriptive_stats()`
    function.
2.  Add median and interquartile range (IQR) to the `summarise()`
    function, by adding it to the end of `list(mean = mean, sd = sd)`,
    after the second `sd`. Note, IQR should look like `iqr = IQR` since
    we want the output columns to have a lowercase for the column names.
3.  Run `tar_outdated()` and `tar_visnetwork()` in the Console (or by
    using the Command Palette {{< var keybind.palette >}}, then "targets
    outdated" or "targets visual"). What does it show?
4.  Style using {{< var keybind.styler >}}.
5.  Run `tar_make()` in the Console or with
    {{< var keybind.targets-make >}}. Re-check for outdated targets and
    visualize the network again.
6.  Open up the Git interface and commit the changes to the Git history
    with {{< var keybind.git >}}.

```{r solution-new-function-descriptive-stats}
#| code-fold: true
#| code-summary: "**Click for a potential solution**. Only click if you are struggling or are out of time."
#' Calculate descriptive statistics of each metabolite.
#'
#' @param data Lipidomics dataset.
#'
#' @return A data.frame/tibble.
#'
descriptive_stats <- function(data) {
  data |>
    dplyr::group_by(metabolite) |>
    dplyr::summarise(dplyr::across(value, list(
      mean = mean,
      sd = sd,
      median = median,
      iqr = IQR
    ))) |>
    dplyr::mutate(dplyr::across(tidyselect::where(is.numeric), ~ round(.x, digits = 1)))
}
```
:::

{{< text_snippet sticky_up >}}

## Adding the plot function as pipeline targets

Now, let's add the plot function to the `_targets.R` file. Let's write
this `tar_target()` item within the `list()` inside `_targets.R`. To
make it easier to track things, add `fig_` to the start of the `name`
given.

``` {.r filename="targets.R"}
list(
  ...,
  tar_target(
    name = fig_metabolite_distribution,
    command = plot_distributions(lipidomics)
  )
)
```

First, style the file using {{< var keybind.styler >}}. Next, test that
it works by running `targets::tar_visnetwork()` using
{{< var keybind.targets-vis >}} or running `targets::tar_outdated()`
with {{< var keybind.targets-outdated >}}. You should see that the new
item is "outdated". Then run `targets::tar_make()` using
{{< var keybind.targets-make >}} to update the pipeline. If it all
works, than **commit the changes to the Git history** with
{{< var keybind.git >}}.

```{r purl-only-plot-fns-in-targets}
#| eval: false
#| echo: false
#| purl: true
update_targets_plots <- "
),
tar_target(
  name = fig_metabolite_distribution,
  command = plot_distributions(lipidomics)
)
"
# print_lines("_targets.R")
# -20 to remove the previous `)`
revise_by_line_num(
  path = "_targets.R",
  insert_text = update_targets_plots,
  remove_original_lines = -42:-43,
  insert_at_line = 41
)

styler::style_dir()
targets::tar_visnetwork()
targets::tar_make()
git_ci("DESCRIPTION", "Add ggplot2 to deps")
git_ci("R/functions.R", "Add output switch function, to use for targets")
git_ci("_targets.R", "Update targets with plot item")
```

## Incorporating Quarto targets

Last, but not least, we want to make the final output 3 from
@fig-pipeline-schematic: The Quarto document. Adding a Quarto document
as a target inside `_targets.R` is fairly straightforward. We need to
install the helper package `{tarchetypes}` first, as well as the
`{quarto}` R package (it helps connect with Quarto):

```{r tarchetypes-deps}
#| purl: true
#| filename: Console
use_package("tarchetypes")
use_package("quarto")
```

Then, inside `_targets.R`, uncomment the line where
`library(tarchetypes)` is commented out. The function we need to use to
build the Quarto file is `tar_quarto()` (or `tar_render()` for R
Markdown files), which needs two things: The `name`, like `tar_target()`
needs, and the file path to the Quarto file. Again, like the other
`tar_target()` items, add it to the end of the `list()`. Lets add
`doc/learning.qmd` as a pipeline step:

``` {.r filename="targets.R"}
list(
  ...,
  tar_quarto(
    name = quarto_doc,
    path = "doc/learning.qmd"
  )
)
```

Then, style the file using {{< var keybind.styler >}}. Now when we run
`targets::tar_make()` with {{< var keybind.targets-make >}}, the Quarto
file also gets re-built. But when we use `targets::tar_visnetwork()`
using {{< var keybind.targets-vis >}}, we don't see the connections with
plot and descriptive statistics. That's because we haven't used them in
a way `{targets}` can recognize. For that, we need to use the function
`targets::tar_read()`. But because our Quarto file is located in the
`doc/` folder and since `{targets}` by default looks in the current
folder for the targets "store" (stored objects that
`targets::tar_read()` looks for) that is kept in the `_targets/` folder,
when we render the Quarto file it won't find the store. So we have to
tell `{targets}` where it is located by using
`targets::tar_config_set()`.

Let's open up the `doc/learning.qmd` file, add a `setup` code chunk
below the YAML header, and create a new header and code chunk and make
use of the `targets::tar_read()`.

<!-- Because the Quarto file is in another folder, -->

```` {.markdown filename="doc/learning.qmd"}
---
# YAML header
---

```{{r setup}}
targets::tar_config_set(store = here::here("_targets"))
library(tidyverse)
library(targets)
source(here::here("R/functions.R"))
lipidomics <- tar_read(lipidomics)
```

## Results

```{{r}}
tar_read(df_stats_by_metabolite)
```

```{{r}}
tar_read(fig_metabolite_distribution)
```
````

```{r purl-only-basic-stats-markdown-text}
#| eval: false
#| echo: false
#| purl: true
basic_stats_md_text <- c(
  "```{r setup}",
  "library(tidyverse)",
  "library(targets)",
  "lipidomics <- tar_read(lipidomics)",
  "```",
  "",
  "## Results",
  "",
  "```{r}",
  "tar_read(df_stats_by_metabolite)",
  "```",
  "",
  "```{r}",
  "tar_read(fig_metabolites_distribution)",
  "```"
)
print_file("doc/learning.qmd")
revise_by_line_num(
  "doc/learning.qmd",
  basic_stats_md_text,
  # Update numbers
  remove_original_lines = -30,
  insert_at_line = 29
)
git_ci("doc/learning.qmd", "Add code for basic stats to report.")
```

When we use `targets::tar_config_set(store = ...)`, it will create a new
file in the `doc/` folder called `_targets.yaml` that contains details
for telling `{targets}` where to find the store. Since the path listed
in this new file is an absolute path, it will only ever work on your own
computer. So, it improve reproducibility, it's good practice to not put
it into the Git history and instead put it in the `.gitignore` file. So
let's add it to ignore file by using:

```{r git-ignore-targets-yaml}
#| filename: Console
use_git_ignore("_targets.yaml")
```

Before continuing, let's commit these changes to the Git history with
{{< var keybind.git >}}.

Now, going back to the `doc/learning.qmd` file, by using
`targets::tar_read()`, we can access all the stored target items using
syntax like we would with `{dplyr}`, without quotes. For the
`df_stats_by_metabolite`, we can do some minor wrangling with `mutate()`
and `glue::glue()`, and than pipe it to `knitr::kable()` to create a
table in the output document. The `{glue}` package is really handy for
formatting text based on columns. If you use `{}` inside a quoted
string, you can use columns from a data frame, like `value_mean`. So we
can use it to format the final table text to be `mean value (SD value)`:

```{r stats-to-table}
#| filename: "doc/learning.qmd"
targets::tar_read(df_stats_by_metabolite) |>
  mutate(MeanSD = glue::glue("{value_mean} ({value_sd})")) |>
  select(Metabolite = metabolite, `Mean SD` = MeanSD) |>
  knitr::kable(caption = "Descriptive statistics of the metabolites.")
```

```{r purl-only-stats-to-table}
#| eval: false
#| echo: false
#| purl: true
pretty_basic_stats_code <- '
targets::tar_read(df_stats_by_metabolite) |>
  mutate(MeanSD = glue::glue("{value_mean} ({value_sd})")) |>
  select(Metabolite = metabolite, `Mean SD` = MeanSD) |>
  knitr::kable(caption = "Descriptive statistics of the metabolites.")
'
revise_by_text(
  path = "doc/learning.qmd",
  original = "tar_read\\(df_stats_by_metabolite\\)",
  replacement = pretty_basic_stats_code
)
styler::style_file("doc/learning.qmd")
git_ci("doc/learning.qmd", "Basic stats as a pretty table.")
```

```{r}
#| eval: true
#| echo: false
lipidomics |>
  descriptive_stats() |>
  mutate(MeanSD = glue::glue("{value_mean} ({value_sd})")) |>
  select(Metabolite = metabolite, `Mean (SD)` = MeanSD) |>
  knitr::kable(caption = "The mean and standard deviation of metabolites in the lipidomics dataset.")
```

Rerun `targets::tar_visnetwork()` using {{< var keybind.targets-vis >}}
to see that it now detects the connections between the pipeline targets.
Then, run `targets::tar_make()` with {{< var keybind.targets-make >}}
again to see everything re-build! Last things are to re-style using
{{< var keybind.styler >}}, then **commit** the changes to the Git
history before moving on with {{< var keybind.git >}}. Then push your
changes up to GitHub.

## :book: Reading task: Fixing issues in the stored pipeline data

**Time: ~10 minutes.**

Sometimes you need to start from the beginning and clean everything up
because there's an issue that you can't seem to fix. In this case,
`{targets}` has a few functions to help out. Here are four that you can
use to delete stuff (also [described on the targets
book](https://books.ropensci.org/targets/data.html#cleaning-up-local-internal-data-files)):

`tar_invalidate()`

:   This removes the metadata on the target in the pipeline, but doesn't
    remove the object itself (which `tar_delete()` does). This will tell
    `{targets}` that the target is out of date, since it has been
    removed, even though the data object itself isn't present. You can
    use this like you would `select()`, by naming the objects directly
    or using the `{tidyselect}` helpers (e.g. `everything()`,
    `starts_with()`).

`tar_delete()`

:   This deletes the stored objects (e.g. the `lipidomics` or
    `df_stats_by_metabolite`) inside `_targets/`, but does not delete
    the record in the pipeline. So `{targets}` will see that the
    pipeline doesn't need to be rebuilt. This is useful if you want to
    remove some data because it takes up a lot of space, or, in the case
    of GDPR and privacy rules, you don't want to store any sensitive
    personal health data in your project. Use it like
    `tar_invalidate()`, with functions like `everything()` or
    `starts_with()`.

`tar_prune()`

:   This function is useful to help clean up left over or unused objects
    in the `_targets/` folder. You will probably not use this function
    too often.

`tar_destroy()`

:   The most destructive, and probably more commonly used, function.
    This will delete the entire `_targets/` folder for those times when
    you want to start over and rerun the entire pipeline again.

{{< text_snippet sticky_up >}}

## Summary

-   Use a function-oriented workflow together with `{targets}` to build
    your data analysis pipeline and track your pipeline "targets".
-   List individual "pipeline targets" using `tar_target()` within the
    `_targets.R` file.
-   Visualize target items in your pipeline with
    `targets::tar_visnetwork()` or list outdated items with
    `targets::tar_outdated()`.
-   Within R Markdown / Quarto files, use `targets::tar_read()` to
    access saved pipeline outputs. To include the Quarto in the
    pipeline, use `{tarchetypes}` and the function `tar_quarto()`.
-   Delete stored pipeline output with `tar_delete()`.

{{< include /includes/_code-appendix.qmd >}}
