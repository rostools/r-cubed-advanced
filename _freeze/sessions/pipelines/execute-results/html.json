{
  "hash": "501920a27d8ec7953da028695acfd783",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute: \n  eval: false\n---\n\n\n\n\n# Creating automatic analysis pipelines {#sec-pipelines}\n\n\n\n\n\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nBefore beginning, get them to recall what they remember of the previous\nsession, either with something like Mentimeter or verbally. Preferably\nsomething like Mentimeter because it allows everyone to participate, not\njust the ones who are more comfortable being vocal to the whole group.\n:::\n\nDepending on how much time you've spent working on data analyses, you\nhave probably experienced (many) times where you are working on a\nproject and forget what code needs to be run first, what order other\ncode needs to run in, and what pieces of code need to be re-run in order\nto update other results. Things get confusing quickly, even for fairly\nsimple projects. This probably happens most often when you return to a\nproject after a month or two and completely forget the state of the\nproject and analysis.\n\nThis is where formal data analysis pipeline tools come in and play a\nrole. By setting up your analysis into distinct steps, with clear inputs\nand outputs, and use a system that tracks those inputs and outputs, you\ncan make things a lot easier for yourself and others. This session is\nabout applying tools that make and manage these pipelines.\n\n## Learning objectives\n\nThe overall objective for this session is to:\n\n1.  Identify and apply an approach to create an analysis pipeline that\n    makes your analysis steps, from raw data to finished manuscript,\n    explicitly defined so that updating it by either you or\n    collaborators is as easy as running a single function.\n\nMore specific objectives are to:\n\n1.  Describe the computational meaning of pipeline and how pipelines are\n    often done in research. Explain why a well-designed pipeline can\n    streamline your collaboration, reduce time spent doing an analysis,\n    make your analysis steps explicit and easier to work on, and\n    ultimately contribute to more fully reproducible research.\n2.  Explain the difference between a \"function-oriented\" workflow vs a\n    \"script-oriented\" workflow and why the function-based approach has\n    multiple advantages from a time- and effort-efficiency point of\n    view.\n3.  Use the functions within `{targets}` to apply the concepts of\n    building pipelines in your analysis project.\n4.  Continue applying the concepts and functions used from the previous\n    session.\n\n## Exercise: How do you re-run analyses when something changes?\n\n> Time: \\~12 minutes.\n\nWe've all been in situations where something in our analysis needs to\nchange. Maybe we forgot to remove a certain condition (like unrealistic\nBMI). Or maybe our supervisor suggests something we hadn't considered in\nthe analysis. Or maybe during peer review of our manuscript, a reviewer\nmakes a suggestion that would improve the understanding of the paper.\nWhatever the situation, we inevitably need to re-run our analysis. And\ndepending on what the change was, we might need to run the full analysis\nall over again. So what is your **exact** workflow when you need to\nre-run code and update your results? Assume it's a change somewhere\nearly in the data processing stage.\n\n1.  Take about 1 minute to think about the workflow you use. Try to\n    think of the *exact* steps you need to take, what *exactly* you do,\n    and how long that usually takes.\n2.  For 8 min, in your group share and discuss what you've thought. How\n    do your experiences compare to each other?\n3.  For the remaining time, we'll briefly share with everyone about what\n    they've thought and discussed.\n\n## What is a data analysis \"pipeline\"?\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nAfter they finish reading this section, briefly walk through it. In\nparticular, emphasize what we want to make at the end, even though that\ngoal might change as the analysis progresses.\n:::\n\n::: callout-note\n## Reading task: \\~10 minutes\n\nA pipeline can be any process where the steps between a start and an end\npoint are very clear, explicit, and concrete. These highly distinct\nsteps can be manual, human involved or can be completely automated by a\nrobot or computer. For instance, in car factories, the pipeline from the\ninput raw materials to the output vehicle are extremely well described\nand implemented. Or like during the pandemic, the pipeline for testing\n(at least in Denmark and several other countries) was highly structured\nand clear for the workers doing the testing and the people having the\ntest done: A person goes in, scans their ID card, has the test done,\nworker inputs the results, results get sent immediately to the health\nagency as well as to the person based on their ID contact information\n(or via a secure app).\n\nHowever, in research, especially around data collection and analysis, we\noften hear or read about \"pipelines\". But looking closer, these aren't\nactual pipelines because the individual steps are not very clear and not\nwell described, often requiring a fair amount of manual human attention\nand intervention. Particularly within computational environments, a\npipeline is when there is minimal to *no* human intervention from raw\ninput to finished output. Why aren't these data \"pipelines\" in research\nactual pipelines? Because:\n\n1.  Anything with data ultimately must be on the computer,\n2.  Anything automatically done on the computer must be done with code,\n3.  Not all researchers write code,\n4.  Researchers who do write code rarely publish and share it,\n5.  Code that is shared or published (either publicly or within the\n    research group) is not written in a way that allows a pipeline to\n    exist,\n6.  And, research is largely non-reproducible [@Trisovic2022;\n    @Seibold2021; @Laurinavichyute2021].\n\n<!-- FIXME: The Seibold ref doesn't work... -->\n\nA data analysis pipeline would by definition be a reproducible,\nreadable, and code-based data analysis. We researchers as a group have a\nlong way to go before we can start realistically implementing data\nanalysis pipelines.\n\nThis isn't to diminish the work of researchers, but a basic observation\non the systemic, social, and structural environment surrounding us. We\nas researchers are not trained in writing code, nor do we have a strong\nculture and incentive structure around learning, sharing, reviewing, and\nimproving code. Nor are we often allowed to get (or use) funds to hire\npeople who *are* trained and skilled in programmatic thinking and\nprogramming. Otherwise courses like this wouldn't need to exist :shrug:\n\nSo how would a data analysis pipeline look? Before we get to that\nthough, we have to separate two things: exploratory data analysis and\nfinal paper data analysis. In exploratory data analysis, there will\nlikely be a lot of manual, interactive steps involved that may or may\nnot need to be explicitly stated and included in the analysis plan and\npipeline. But for the final paper and what results would be included, we\ngenerally have some basic first ideas. Let's list a few items that we\nwould want to do before the primary statistical analysis:\n\n1.  A table of some basic descriptive statistics of the study\n    population, such as mean, standard deviation, or counts of basic\n    discrete data (like treatment group).\n2.  A figure showing the distribution of your main variables of\n    interest. In this case, ours are the lipidomic variables.\n3.  The paper with the results included.\n\n\n\n\n```{mermaid}\n%%| label: fig-pipeline-schematic\n%%| fig-cap: Very simple flow chart of steps in an analysis pipeline.\n%%| echo: false\n%%| eval: true\n%%{init:{'flowchart':{'nodeSpacing': 20, 'rankSpacing':30}}}%%\ngraph LR\n    data[Data] --> fn_desc{{function}} \n    fn_desc --> tab_desc[Output 1:<br>Descriptive<br>statistics<br>table]\n    tab_desc --> paper[Output 3:<br>Paper]\n    data --> fn_plot_vars{{function}}\n    fn_plot_vars{{function}} --> plot_distrib[Output 2:<br>Continuous<br>variables<br>plot]\n    plot_distrib --> paper\n\nlinkStyle 0,1,2,3,4,5 stroke-width:1px;\n```\n\n\n\n\nNow that we conceptually have drawn out the sets of tasks to complete in\nour pipeline, we can start using R to build it.\n:::\n\n## Using targets to manage the pipeline\n\nThere are a few packages to help build pipelines in R, but the most\ncommonly used, well-designed, and maintained one is called `{targets}`.\nWith this package, you specify outputs you want to create and\n`{targets}` will track them for you. So it will know which output\ndepends on which other one and which ones need to be updated.\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nAsk participants, which do they think it is: a build dependency or a\nworkflow dependency. Because it is directly used to run analyses and\nprocess the data, it would be a build dependency.\n:::\n\nFirst, we need to install `{targets}` in the project environment. And\nsince `{targets}` is a build dependency, we add it to the `DESCRIPTION`\nfile with:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuse_package(\"targets\")\n```\n:::\n\n\n\n\nNow that it's added to the project R library, let's set up our project\nto start using it!\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntargets::use_targets()\n```\n:::\n\n\n\n\nThis will add several files:\n\n```         \n.\n├── _targets.R\n├── run.R\n└── run.sh\n```\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nThe most important file is the `_targets.R` file. The other two files\nare used for other situations (like running on a Linux server) that we\nwon't cover in the course. Before we continue though, let's **commit\nthese new files to the Git history** with {{< var keybind.git >}}.\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nLet them read it before going over it again to reinforce\nfunction-oriented workflows and how `{targets}` and the `tar_target()`\nworks.\n:::\n\n:::: callout-note\n## Reading task: \\~8\n\nNext, open up the `_targets.R` and we'll take a look at what it\ncontains.\n\n::: {.callout-note collapse=\"true\"}\n## Click this to see the file contents\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # Created by use_targets().\n#> # Follow the comments below to fill in this target script.\n#> # Then follow the manual to check and run the pipeline:\n#> #   https://books.ropensci.org/targets/walkthrough.html#inspect-the-pipeline\n#> \n#> # Load packages required to define the pipeline:\n#> library(targets)\n#> # library(tarchetypes) # Load other packages as needed.\n#> \n#> # Set target options:\n#> tar_option_set(\n#>   packages = c(\"tibble\") # Packages that your targets need for their tasks.\n#>   # format = \"qs\", # Optionally set the default storage format. qs is fast.\n#>   #\n#>   # Pipelines that take a long time to run may benefit from\n#>   # optional distributed computing. To use this capability\n#>   # in tar_make(), supply a {crew} controller\n#>   # as discussed at https://books.ropensci.org/targets/crew.html.\n#>   # Choose a controller that suits your needs. For example, the following\n#>   # sets a controller that scales up to a maximum of two workers\n#>   # which run as local R processes. Each worker launches when there is work\n#>   # to do and exits if 60 seconds pass with no tasks to run.\n#>   #\n#>   #   controller = crew::crew_controller_local(workers = 2, seconds_idle = 60)\n#>   #\n#>   # Alternatively, if you want workers to run on a high-performance computing\n#>   # cluster, select a controller from the {crew.cluster} package.\n#>   # For the cloud, see plugin packages like {crew.aws.batch}.\n#>   # The following example is a controller for Sun Grid Engine (SGE).\n#>   # \n#>   #   controller = crew.cluster::crew_controller_sge(\n#>   #     # Number of workers that the pipeline can scale up to:\n#>   #     workers = 10,\n#>   #     # It is recommended to set an idle time so workers can shut themselves\n#>   #     # down if they are not running tasks.\n#>   #     seconds_idle = 120,\n#>   #     # Many clusters install R as an environment module, and you can load it\n#>   #     # with the script_lines argument. To select a specific verison of R,\n#>   #     # you may need to include a version string, e.g. \"module load R/4.3.2\".\n#>   #     # Check with your system administrator if you are unsure.\n#>   #     script_lines = \"module load R\"\n#>   #   )\n#>   #\n#>   # Set other options as needed.\n#> )\n#> \n#> # Run the R scripts in the R/ folder with your custom functions:\n#> tar_source()\n#> # tar_source(\"other_functions.R\") # Source other scripts as needed.\n#> \n#> # Replace the target list below with your own:\n#> list(\n#>   tar_target(\n#>     name = data,\n#>     command = tibble(x = rnorm(100), y = rnorm(100))\n#>     # format = \"qs\" # Efficient storage for general data objects.\n#>   ),\n#>   tar_target(\n#>     name = model,\n#>     command = coefficients(lm(y ~ x, data = data))\n#>   )\n#> )\n```\n\n\n:::\n:::\n\n\n\n:::\n\nNext, notice `tar_target()` function used at the end of the script.\nThere are two main arguments for it `name` and `command`. The way that\n`{targets}` works is similar to how you'd write R code to assign the\noutput of a function to an object.\n\n``` r\nobject_name <- function_in_command(input_arguments)\n```\n\nIs the same as:\n\n``` r\ntar_target(\n  name = object_name,\n  command = function_in_command(input_arguments)\n)\n```\n\nWhat this means is that `{targets}` follows a [\"function-oriented\"\nworkflow](https://books.ropensci.org/targets/functions.html#problems-with-script-based-workflows),\nnot a \"script-based\" workflow. What's the difference? In a\nscript-oriented workflow, each R file is run in a specific order, so you\nmight end up with an R file that has code like:\n\n``` r\nsource(\"R/1-process-data.R\")\nsource(\"R/2-basic-statistics.R\")\nsource(\"R/3-create-plots.R\")\nsource(\"R/4-linear-regression.R\")\n```\n\nWhile in a function-oriented workflow, it might look more like:\n\n``` r\nsource(\"R/functions.R\")\nraw_data <- load_raw_data(\"file/path/data.csv\")\nprocessed_data <- process_data(raw_data)\nbasic_stats <- calculate_basic_statistics(processed_data)\nsimple_plot <- create_plot(processed_data)\nmodel_results <- run_linear_reg(processed_data)\n```\n\nWith this workflow, each function takes an input dataset and contains\nall the code to create the results into one output, like a figure in a\npaper. If you've taken the [intermediate R\ncourse](https://r-cubed-intermediate.rostools.org/), you'll notice that\nthis function-oriented workflow is the workflow we covered. There are so\nmany advantages to this type of workflow and is the reason many powerful\nR packages are designed around making use of this type workflow.\n\nIf we take these same code and convert it into the `{targets}` format,\nthe end of `_targets.R` file would like this:\n\n``` r\nlist(\n  tar_target(\n    name = raw_data,\n    command = load_raw_data(\"file/path/data.csv\")\n  ),\n  tar_target(\n    name = processed_data,\n    command = process_data(raw_data)\n  ),\n  tar_target(\n    name = basic_stats,\n    command = calculate_basic_statistics(processed_data)\n  ),\n  tar_target(\n    name = simple_plot,\n    command = create_plot(processed_data)\n  ),\n  tar_target(\n    name = model_results,\n    command = run_linear_reg(processed_data)\n  )\n)\n```\n::::\n\nLet's start writing code to create the three items we listed above in\n@fig-pipeline-schematic: some descriptive statistics, a plot of the\ncontinuous lipid variables, and a report (R Markdown). Since we'll use\n`{tidyverse}`, specifically `{dplyr}`, to calculate the summary\nstatistics, we need to add it to our dependencies and install it in the\nproject library. `{tidyverse}` is a special \"meta\"-package so we need to\nadd it to the `\"depends\"` section of the `DESCRIPTION` file.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuse_package(\"tidyverse\", \"depends\")\n```\n:::\n\n\n\n\nCommit the changes made to the `DESCRIPTION` file in the Git history\nwith {{< var keybind.git >}}.\n\nNow, let's start doing some data analysis so that we can add to our\npipeline later on. First, open up the `doc/learning.qmd` file and create\na new header and code chunk at the bottom of the file.\n\n````         \n\n```{{r setup}}\nlibrary(tidyverse)\nsource(here::here(\"R/functions.R\"))\nlipidomics <- read_csv(here::here(\"data/lipidomics.csv\"))\n```\n\n## Basic statistics\n\n```{{r basic-stats}}\n\n```\n````\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nWe want to calculate the mean and SD for each metabolite and then, to\nmake it more readable, to round the numbers to one digit. We covered\nthis in the\n[functionals](https://r-cubed-intermediate.rostools.org/dry-functionals.html#summarizing-long-data-like-the-rr-dataset)\nsession of the intermediate course, so we will apply these same\nprinciples and code here. To do that, we need to use `group_by()` on the\n`metabolites`, use `across()` inside `summarise()` so we can give it the\n`mean()` and `sd()` functions, followed by `mutate()`ing each numeric\ncolumn (`across(where(is.numeric))`) to `round()` to 1 digit. Let's\nwrite out the code!\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlipidomics %>%\n  group_by(metabolite) %>%\n  summarise(across(value, list(mean = mean, sd = sd))) %>%\n  mutate(across(where(is.numeric), ~round(.x, digits = 1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 12 × 3\n#>    metabolite               value_mean value_sd\n#>    <chr>                         <dbl>    <dbl>\n#>  1 CDCl3 (solvent)               180       67  \n#>  2 Cholesterol                    18.6     11.4\n#>  3 FA -CH2CH2COO-                 33.6      7.8\n#>  4 Lipid -CH2-                   537.      61.9\n#>  5 Lipid CH3- 1                   98.3     73.8\n#>  6 Lipid CH3- 2                  168.      29.2\n#>  7 MUFA+PUFA                      32.9     16.1\n#>  8 PUFA                           30       24.1\n#>  9 Phosphatidycholine             31.7     20.5\n#> 10 Phosphatidylethanolamine       10        7.6\n#> 11 Phospholipids                   2.7      2.6\n#> 12 TMS (interntal standard)      123      130.\n```\n\n\n:::\n:::\n\n\n\n\nAfter that, style the file using {{< var keybind.styler >}} on the file.\nThan we will **commit** the changes to the Git history with\n{{< var keybind.git >}}.\n\n## Exercise: Convert summary statistics code into a function\n\n> Time: \\~20 minutes.\n\nWhile inside the `doc/learning.qmd` file, use the \"function-oriented\"\nworkflow, as taught in the [intermediate\ncourse](https://r-cubed-intermediate.rostools.org/dry-functions.html#the-basics-of-a-function),\nto take the code we wrote above and convert it into a function. Complete\nthese tasks:\n\n1.  Wrap the code with `function() {...}` and name the new function\n    `descriptive_stats`.\n2.  Replace `lipidomics` with `data` and put `data` as an argument\n    inside the brackets of `function()`.\n3.  Add `dplyr::` to the start of each `{dplyr}` function used inside\n    your function (except for `where()`, which comes from the\n    `{tidyselect}` package).\n4.  Style the code using {{< var keybind.styler >}} to make sure it is\n    formatted correctly. You might need to manually force a styling if\n    lines are too long.\n5.  With the *cursor* inside the function, add some roxygen\n    documentation with {{< var keybind.palette >}} followed by typing\n    \"roxygen comment\". Remove the lines that contain `@examples` and\n    `@export`, then fill in the other details (like the `@params` and\n    `Title`). In the `@return` section, write \"A data.frame/tibble.\"\n6.  Cut and paste the function over into the `R/functions.R` file.\n7.  Source the `R/functions.R` file with {{< var keybind.source >}}, and\n    then test the code by running `descriptive_stats(lipidomics)` in the\n    Console. If it works, do the last task.\n8.  Save both files and then open the Git interface and commit the\n    changes you made to them with {{< var keybind.git >}}.\n\n::: {.callout-tip appearance=\"default\"}\nIn the intermediate course, we highly suggested using `return()` at the\nend of the function. Technically we don't need an explicit `return()`,\nsince the output of the last code that R runs within the function will\nbe the output of the function. This is called an \"implicit return\" and\nwe will be using this feature throughout the rest of this course.\n:::\n\nHere is some scaffolding to help you get started:\n\n``` r\ndescriptive_stats <- function(___) {\n  ___\n}\n```\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"**Click for a potential solution**. Only click if you are struggling or are out of time.\"}\n#' Calculate descriptive statistics of each metabolite.\n#'\n#' @param data The lipidomics dataset.\n#'\n#' @return A data.frame/tibble.\n#'\ndescriptive_stats <- function(data) {\n  data %>%\n    dplyr::group_by(metabolite) %>%\n    dplyr::summarise(dplyr::across(value, list(mean = mean, sd = sd))) %>%\n    dplyr::mutate(dplyr::across(tidyselect::where(is.numeric), ~round(.x, digits = 1)))\n}\n```\n:::\n\n\n\n\n## Adding a step in the pipeline\n\nNow that we've created a function to calculate some basic statistics, we\ncan now add it as a step in the `{targets}` pipeline. Open up the\n`_targets.R` file and go to the end of the file, where the `list()` and\n`tar_target()` code are found. In the first `tar_target()`, replace the\ntarget to load the lipidomic data. In the second, replace it with the\n`descriptive_stats()` function. If we want to make it easier to remember\nwhat the target output is, we can add `df_` to remind us that it is a\ndata frame. It should look like:\n\n``` r\nlist(\n  tar_target(\n    name = lipidomics,\n    command = readr::read_csv(here::here(\"data/lipidomics.csv\"))\n  ),\n  tar_target(\n    name = df_stats_by_metabolite,\n    command = descriptive_stats(lipidomics)\n  )\n)\n```\n\nLet's run `{targets}` to see what happens! You can either use\n{{< var keybind.targets-make >}} or run this code in the Console:\n\n``` r\ntargets::tar_make()\n```\n\n<!-- TODO: Check how this works, and update next section -->\n\nWhile this `{targets}` pipeline works, we would not be able to\nautomatically re-run our pipeline if our underlying data changes. To\ntrack the actual data file we need to create a pipeline target of the\ndata file. We can accomplish this by using the argument\n`format = \"file\"` inside the `tar_target()` before loading the data\nusing `{readr}`.\n\n``` r\nlist(\n    tar_target(\n        name = file,\n        command = \"data/lipidomics.csv\",\n        format = \"file\"\n    ),\n    tar_target(\n        name = lipidomics,\n        command = readr::read_csv(file, show_col_types = FALSE)\n    ),\n    tar_target(\n        name = df_stats_by_metabolite,\n        command = descriptive_stats(lipidomics)\n    )\n)\n```\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nNow, let's try running `{targets}` again using\n{{< var keybind.targets-make >}}.\n\nIt probably won't run though. That's because `{targets}` doesn't know\nabout the packages that you need for the pipeline. To add it, we need to\ngo to the `tar_option_set()` section of the `_targets.R` file and add to\nthe `packages = c(\"tibble\")` code with the packages we use that aren't\nexplicitly called via `::` (e.g. `%>%`). For now, we only need to add\n`\"dplyr\"` to the `packages` argument.\n\nWe can now put this code in the `packages` argument of\n`tar_option_set()` in the `_targets.R` file:\n\n``` r\npackages = c(\"tibble\", \"dplyr\")\n```\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nTry running `{targets}` again with either `targets::tar_make()` or\n{{< var keybind.targets-make >}}. It should run through! We also see\nthat a new folder has been created called `_targets/`. Inside this\nfolder it will keep all of the output from running the code. It comes\nwith i's own `.gitignore` file so that you don't track all the files\ninside, since they aren't necessary. Only the `_targets/meta/meta` is\nneeded to include in Git.\n\nWe can visualize our individual pipeline targets that we track through\n`tar_target()` now too, which can be useful as you add more and more\ntargets. We will (likely) need to install an extra package (done\nautomatically):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntargets::tar_visnetwork()\n```\n:::\n\n\n\n\nOr to see what pipeline targets are outdated:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntargets::tar_outdated()\n```\n:::\n\n\n\n\nBefore continuing, let's commit the changes (including the files in the\n`_targets/` folder) to the Git history with {{< var keybind.git >}}.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Creating figure outputs\n\nNot only can we create data frames with targets (like above), but also\nfigures. Let's write some code to create the plot we listed as our\n\"output 2\" in @fig-pipeline-schematic. Since we're using `{ggplot2}` to\nwrite this code, let's add it to our `DESCRIPTION` file.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuse_package(\"ggplot2\")\n```\n:::\n\n\n\n\nNext, we'll switch back to `doc/lesson.qmd` and write the code to this\nplot of the distribution of each metabolite. We'll use\n`geom_histogram()`, nothing too fancy. And since the data is already in\nlong format, we can easily use `facet_wrap()` to create a plot for each\nmetabolite. We use `scales = \"free\"` because each metabolite doesn't\nhave the same range of values (some are small, others are quite large).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmetabolite_distribution_plot <- ggplot(lipidomics, aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(vars(metabolite), scales = \"free\")\nmetabolite_distribution_plot\n```\n\n::: {.cell-output-display}\n![Histograms showing the distribution of all metabolites in the lipidomics dataset.](pipelines_files/figure-html/histogram-metabolites-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nWe now have the basic code to convert over into functions.\n\n## Exercise: Convert the plot code to a function\n\n> Time: \\~10 minutes.\n\nFor now, we will only take the code to make the distribution plot and\nconvert it into a function. Just like you did with the\n`descriptive_stats()` function in the exercise above, complete these\ntasks:\n\n1.  Wrap the plot code inside `doc/lesson.qmd` with `function() {...}`\n    and name the new function `plot_distributions`.\n2.  Replace `lipidomics` with `data` and put `data` as an argument\n    inside the brackets of `function()`.\n3.  Add `ggplot2::` to the start of each `{ggplot2}` function used\n    inside your function.\n4.  Style using {{< var keybind.styler >}} to make sure it is formatted\n    correctly. You might need to manually force a styling if lines are\n    too long.\n5.  With the *cursor* inside the function, add some roxygen\n    documentation with {{< var keybind.roxygen >}}. Remove the lines\n    that contain `@examples` and `@export`, then fill in the other\n    details (like the `@params` and `Title`). In the `@return` section,\n    write \"A plot object.\"\n6.  Cut and paste the function over into the `R/functions.R` file.\n7.  Source the `R/functions.R` file ({{< var keybind.source >}}) and\n    then test the code by running `plot_distributions(lipidomics)` in\n    the Console. If it works, do the last task.\n8.  Save both files and then open the Git interface and commit the\n    changes you made to them with {{< var keybind.git >}}.\n\nUse this scaffolding code to help guide you to write the code into a\nfunction.\n\n``` r\nplot_distributions <- function(___) {\n  ___\n}\n```\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"**Click for the solution**. Only click if you are struggling or are out of time.\"}\n## This should be in the R/functions.R file.\n#' Plot for basic distribution of metabolite data.\n#'\n#' @param data The lipidomics dataset.\n#'\n#' @return A ggplot2 graph.\n#'\nplot_distributions <- function(data) {\n  data %>% \n    ggplot2::ggplot(ggplot2::aes(x = value)) +\n    ggplot2::geom_histogram() +\n    ggplot2::facet_wrap(ggplot2::vars(metabolite), scales = \"free\")\n}\n```\n:::\n\n\n\n\n## Adding the plot function as pipeline targets\n\nNow, let's add the plot function to the `_targets.R` file. Let's write\nthis `tar_target()` item within the `list()` inside `_targets.R`. To\nmake it easier to track things, add `fig_` to the start of the `name`\ngiven.\n\n``` r\nlist(\n  ...,\n  tar_target(\n    name = fig_metabolite_distribution,\n    command = plot_distributions(lipidomics)\n  )\n)\n```\n\nNext, test that it works by running `targets::tar_visnetwork()` using\n{{< var keybind.targets-vis >}} or running `targets::tar_outdated()`\nwith {{< var keybind.targets-outdated >}}. You should see that the new\nitem is \"outdated\". Then run `targets::tar_make()` using\n{{< var keybind.targets-make >}} to update the pipeline. If it all\nworks, than **commit the changes to the Git history** with\n{{< var keybind.git >}}.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Incorporating Quarto targets\n\nLast, but not least, we want to make the final output 3 from\n@fig-pipeline-schematic: The Quarto document. Adding a Quarto document\nas a target inside `_targets.R` is fairly straightforward. We need to\ninstall the helper package `{tarchetypes}` first, as well as the\n`{quarto}` R package (it helps connect with Quarto):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuse_package(\"tarchetypes\")\nuse_package(\"quarto\")\n```\n:::\n\n\n\n\nThen, inside `_targets.R`, uncomment the line where\n`library(tarchetypes)` is commented out. The function we need to use to\nbuild the Quarto file is `tar_quarto()` (or `tar_render()` for R\nMarkdown files), which needs two things: The `name`, like `tar_target()`\nneeds, and the file path to the Quarto file. Again, like the other\n`tar_target()` items, add it to the end of the `list()`. Since we're\nusing the `doc/learning.qmd` as a sandbox, we won't include it as a\npipeline target. Instead we will use the `doc/learning.qmd` file:\n\n``` r\nlist(\n  ...,\n  tar_quarto(\n    name = quarto_doc, \n    path = \"doc/learning.qmd\"\n  )\n)\n```\n\nNow when we run `targets::tar_make()` with\n{{< var keybind.targets-make >}}, the Quarto file also gets re-built.\nBut when we use `targets::tar_visnetwork()` using\n{{< var keybind.targets-vis >}}, we don't see the connections with plot\nand descriptive statistics. That's because we haven't used them in a way\n`{targets}` can recognize. For that, we need to use the function\n`targets::tar_read()`. And because our Quarto file is located in the\n`doc/` folder, we also have to tell Quarto where the targets \"store\"\n(stored objects that `targets::tar_read()` looks for), which we do by\nusing `targets::tar_config_set()`.\n\nLet's open up the `doc/learning.qmd` file, add a `setup` code chunk\nbelow the YAML header, and create a new header and code chunk and make\nuse of the `targets::tar_read()`.\n\n<!-- Because the Quarto file is in another folder, -->\n\n````         \n---\n# YAML header\n---\n\n```{{r setup}}\ntargets::tar_config_set(store = here::here(\"_targets\"))\nlibrary(tidyverse)\nlibrary(targets)\nsource(here::here(\"R/functions.R\"))\nlipidomics <- tar_read(lipidomics)\n```\n\n## Results\n\n```{{r}}\ntar_read(df_stats_by_metabolite)\n```\n\n```{{r}}\ntar_read(fig_metabolites_distribution)\n```\n````\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\nWith `targets::tar_read()`, we can access all the stored target items\nusing syntax like we would with `{dplyr}`, without quotes. For the\n`df_stats_by_metabolite`, we can do some minor wrangling with `mutate()`\nand `glue::glue()`, and than pipe it to `knitr::kable()` to create a\ntable in the output document. The `{glue}` package is really handy for\nformatting text based on columns. If you use `{}` inside a quoted\nstring, you can use columns from a data frame, like `value_mean`. So we\ncan use it to format the final table text to be `mean value (SD value)`:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntargets::tar_read(df_stats_by_metabolite) %>% \n  mutate(MeanSD = glue::glue(\"{value_mean} ({value_sd})\")) %>%\n  select(Metabolite = metabolite, `Mean SD` = MeanSD) %>%\n  knitr::kable(caption = \"Descriptive statistics of the metabolites.\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\nTable: The mean and standard deviation of metabolites in the lipidomics dataset.\n\n|Metabolite               |Mean (SD)    |\n|:------------------------|:------------|\n|CDCl3 (solvent)          |180 (67)     |\n|Cholesterol              |18.6 (11.4)  |\n|FA -CH2CH2COO-           |33.6 (7.8)   |\n|Lipid -CH2-              |536.6 (61.9) |\n|Lipid CH3- 1             |98.3 (73.8)  |\n|Lipid CH3- 2             |168.2 (29.2) |\n|MUFA+PUFA                |32.9 (16.1)  |\n|PUFA                     |30 (24.1)    |\n|Phosphatidycholine       |31.7 (20.5)  |\n|Phosphatidylethanolamine |10 (7.6)     |\n|Phospholipids            |2.7 (2.6)    |\n|TMS (interntal standard) |123 (130.4)  |\n\n\n:::\n:::\n\n\n\n\nRe-run `targets::tar_visnetwork()` using {{< var keybind.targets-vis >}}\nto see that it now detects the connections between the pipeline targets.\nThen, run `targets::tar_make()` with {{< var keybind.targets-make >}}\nagain to see everything re-build! Last things are to re-style using\n{{< var keybind.styler >}}, then **commit** the changes to the Git\nhistory before moving on with {{< var keybind.git >}}. Then push your\nchanges up to GitHub.\n\n## Fixing issues in the stored pipeline data\n\n::: callout-note\n## Reading task: \\~10 minutes\n\nSometimes you need to start from the beginning and clean everything up\nbecause there's an issue that you can't seem to fix. In this case,\n`{targets}` has a few functions to help out. Here are four that you can\nuse to delete stuff (also [described on the targets\nbook](https://books.ropensci.org/targets/data.html#cleaning-up-local-internal-data-files)):\n\n`tar_invalidate()`\n\n:   This removes the metadata on the target in the pipeline, but doesn't\n    remove the object itself (which `tar_delete()` does). This will tell\n    `{targets}` that the target is out of date, since it has been\n    removed, even though the data object itself isn't present. You can\n    use this like you would `select()`, by naming the objects directly\n    or using the `{tidyselect}` helpers (e.g. `everything()`,\n    `starts_with()`).\n\n`tar_delete()`\n\n:   This deletes the stored objects (e.g. the `lipidomics` or\n    `df_stats_by_metabolite`) inside `_targets/`, but does not delete\n    the record in the pipeline. So `{targets}` will see that the\n    pipeline doesn't need to be rebuilt. This is useful if you want to\n    remove some data because it takes up a lot of space, or, in the case\n    of GDPR and privacy rules, you don't want to store any sensitive\n    personal health data in your project. Use it like\n    `tar_invalidate()`, with functions like `everything()` or\n    `starts_with()`.\n\n`tar_prune()`\n\n:   This function is useful to help clean up left over or unused objects\n    in the `_targets/` folder. You will probably not use this function\n    too often.\n\n`tar_destroy()`\n\n:   The most destructive, and probably more commonly used, function.\n    This will delete the entire `_targets/` folder for those times when\n    you want to start over and re-run the entire pipeline again.\n:::\n\n## Summary\n\n-   Use a function-oriented workflow together with `{targets}` to build\n    your data analysis pipeline and track your \"pipeline targets\".\n-   List individual \"pipeline targets\" using `tar_target()` within the\n    `_targets.R` file.\n-   Visualize target items in your pipeline with\n    `targets::tar_visnetwork()` or list outdated items with\n    `targets::tar_outdated()`.\n-   Within R Markdown / Quarto files, use `targets::tar_read()` to\n    access saved pipeline outputs. To include the Quarto in the\n    pipeline, use `{tarchetypes}` and the function `tar_quarto()`.\n-   Delete stored pipeline output with `tar_delete()`.\n",
    "supporting": [
      "pipelines_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}