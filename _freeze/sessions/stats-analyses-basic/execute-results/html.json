{
  "hash": "cca2d179e5f9ebfbbb16896eaf8b9774",
  "result": {
    "engine": "knitr",
    "markdown": "# A general approach to doing statistical analyses {#sec-stats-analyses-basic}\n\n\n\n\n\n\n\n\n\nRunning statistical analyses is a relatively methodical and well-defined\nprocess, even if there is often a lot of trial and error involved.\nSometimes it may feel overwhelming and complicated, which it definitely\ncan be, but it doesn't have to be. By taking a structured approach to\nrunning statistical analyses, you can make it easier on yourself and\nfeel more in control.\n\nIn R, statistical methods are often created and developed by researchers\nwith little to no training in software development and who often use\nthem differently. This has some advantages, like having the cutting edge\nstatistical methods available to us, but has a major disadvantage of\noften having to learn a completely different way of running a\nstatistical analysis, even if it is fairly similar to ones you've used\nbefore. So having a framework for running statistical analyses,\nregardless of who created them, can provide that needed structure and\nvastly simplify the analysis. This session will be covering a general\nframework for running statistical analyses, regardless of the exact\nstatistical method.\n\n## Learning objectives\n\nThe overall objective for this session is to:\n\n1.  Describe the basic framework underlying most statistical analyses\n    and use R to generate statistical results using this framework.\n\nMore specific objectives are to:\n\n1.  Describe the general \"workflow\" and steps involved in stating the\n    research question, constructing a model to help answer the question,\n    preparing the data to match the requirements of the model, fitting\n    it to the data, and finally extracting the results from the fitted\n    model.\n2.  Categorize the model definition step as a distinct, theory-driven\n    step, separate from the data, and use `{parsnip}` functions to help\n    with defining your model.\n3.  Identify various data transformation techniques and evaluate which\n    are good options given the data. Use functions in the `{recipes}`\n    package to apply these transformations to your data.\n4.  Use the `{broom}` package to extract the model results to later\n    present them in graphical format (with `{ggplot2}`).\n5.  Continue applying the concepts and functions used from the previous\n    sessions.\n\nSpecific \"anti\"-objectives:\n\n-   Will **not** know how to choose and apply the appropriate\n    statistical model or test, nor understand any statistical theory,\n    nor interpret the statistical results correctly, nor determine the\n    relevant data transformations for the statistical approach. What we\n    show, we show *only as demonstration purposes only*, they could be\n    entirely wrong in how to do them correctly if an expert were to\n    review them.\n\n::: {.callout-warning appearance=\"default\"}\nWe will be making *a lot* of function throughout this session and the\nnext. This is just a fair warning!\n:::\n\n## Exercise: What does a \"model\" mean?\n\n> Time: \\~6 minutes.\n\nIn science and especially statistics, we talk a lot about \"models\". But\nwhat does model actually mean? What different types of definitions can\nyou think of? Is there a different understanding of model in statistics\ncompared to other areas?\n\n1.  Take 1 minute to think about your understanding of a model.\n2.  Then, over the next 3 minutes, discuss with your neighbour about the\n    meaning of \"model\" and see if you can come to a shared\n    understanding.\n3.  Finally, over the next 2 minutes, we will share all together what a\n    model is in the context of data analysis.\n\n## Theory and \"workflow\" on statistical modeling\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nLet them read it over than go over it briefly, focusing on what a model\nis, that we should create research questions while thinking in the\nframework of models, and the general workflow for doing statistical\nanalyses.\n:::\n\n<!-- TODO: Create a revealjs presentation on this? -->\n\n::: callout-note\n## Reading task: \\~15 minutes\n\nAlmost all fields of research, and definitely more heavily-quantitative\nand scientific fields like biomedicine and health, have math and\nstatistics at the core of taking data to draw inferences or general\nobservations about the world.\n\nAny time we collect data and need to interpret what it means, we need\nstatistics. And anytime we want to make inferences about the world from\nthe data, we need to use statistics to determine the likelihood, or\nrather the uncertainty, in those inferences. Statistics is meant to\nquantify uncertainty.\n\nHow do we quantify uncertainty? By first creating a \"theoretical model\"\nthat expresses mathematically our research question. For instance, we\nhave a theoretical model that outdoor plants grow (non-linearly) with\nwater and sunlight, but that more sunlight likely means less water (less\nrain). While *any* research question could be translated into a\ntheoretical model, not all theoretical models can be *tested* against\nthe real world. And that's where the second part comes in: Our\ntheoretical model needs to be structured in a way that allows us to\nmeasure the items (\"parameters\") in our model, so that we can build a\nmathematical model from the data (\"test it in the real world\").\n\n\n\n\n```{mermaid}\n%%| label: fig-model-plant-growth\n%%| fig-cap: Simple example of a theoretical model of plant growth.\n%%| echo: false\n%%| eval: true\n%%{init:{'flowchart':{'nodeSpacing': 20, 'rankSpacing': 30}}}%%\ngraph LR\n    Sunlight --> Growth\n    Water --> Growth\n    Sunlight --- Water\n\nlinkStyle 0,1,2 stroke-width:1px;\n```\n\n\n\n\nGreat research questions are designed in a way to fit a theoretical\nmodel on measurable parameters, so we can ultimately quantify the\nuncertainty in our observations (the data) and in the model. And the\nbasic simplified math of a statistical model looks mostly the same:\n\n$$y = intercept + x + error $$\n\nSo if we use the example of plant growth, it would look like:\n\n$$Growth = Sunlight + Water$$\n\nIt's a bit more complicated than this, but this is enough to describe it\nfor this course. Throughout the rest of the session, we use specific\nterms to describe each item in this formula. \"Outcome\" (also called\n\"dependent variable\") refers to the $y$, \"predictor\" (or \"independent\nvariable\") refers to the $x$. The error and intercept are calculated for\nus when we fit the model to the data. The intercept is when x is equal\nto zero (the \"y-intercept\" on a plot). The error is the difference\nbetween what the model estimates and the real value. It plays a role in\nquantifying the model's uncertainty.\n\nConsidering the mathematical nature of statistical models, there is also\na logic and \"workflow\" to making these models!\n\n1.  Write a research question, designed in a way that might look like\n    the diagram above. Usually this step needs to be revisited, revising\n    the question after trying to construct the theoretical model, that\n    describes the measurable (and unmeasurable) parameters in the model,\n    and vice versa.\n2.  Based on the model and the type of measured parameters used\n    (continuous or binary), select the best mathematical model \"type\".\n    Nearly all models in statistics start from the base of a linear\n    regression (e.g. ANOVA is a special form of regression, t-test is a\n    simpler version of ANOVA), so the model \"type\" will probably be a\n    form of regression.\n3.  Measure your parameters (in the plant growth example, that might be\n    the amount of water given in liters per day, amount of plant growth\n    in weight, and amount of sunlight in hrs per day). Usually, this\n    measured data need to be processed in a special way to fit the\n    specifics of the model, research question, and practices of the\n    field.\n4.  Fit the data to the theoretical model in order to estimate the\n    values (\"coefficients\") of the model parameters as well as the\n    uncertainty in those values.\n5.  Extract the values and their uncertainty from the model and present\n    them in relation to your research questions.\n\n\n\n\n```{mermaid}\n%%| label: fig-model-building-workflow\n%%| fig-cap: Simple schematic of the workflow for conducting statistical analysis.\n%%| echo: false\n%%| eval: true\n%%{init:{'flowchart':{'nodeSpacing': 40, 'rankSpacing': 20}}}%%\ngraph TD\n    A[Research question] --> B((Statistical model))\n    B --> A\n    B --> C[Data collection]\n    C --> D[Data transformation]\n    D --> E[Scientific output]\n    E --> A\n    \nlinkStyle 0,1,2,3,4,5 stroke-width:1px;\n```\n\n\n\n\n::: {.callout-caution appearance=\"default\"}\nThe entire workflow for building statistical models requires highly\nspecific domain knowledge on not only the statistics themselves, but\nalso how the data was collected, what the values mean, what type of\nresearch questions to ask and how to ask them, how to interpret the\nresults from the models, and how to process the data to fit the question\nand model.\n\nFor instance, in our `lipidomics` dataset, if we were to actually use\nthis data, we would need someone familiar with -omic technologies, how\nthe data are measured, what the values actually mean, how to prepare\nthem for the modeling, the specific modeling methods used for this\nfield, and how we would actually interpret the findings. We have\n**none** of these things, so very likely we are doing things quite wrong\nhere. We're only doing this modeling to highlight how to use the R\npackages.\n:::\n\nGoing back to our own `lipidomics` dataset, we need to do the first\nstep: Creating the question. While we don't have much data, there are a\nsurprising number of questions we could ask. But we will keep it very\nsimple, very basic, and very exploratory.\n\n1.  What is the estimated relationship of each metabolite with T1D\n    compared to the controls, adjusting for the influence of age and\n    gender?\n\nNext, because we are working within a \"reproducible analysis\" framework\nspecifically with the use of `{targets}`, let's convert this question\ninto outputs to include as pipeline targets, along with a basic idea for\nthe final functions that will make up these targets and their inputs and\noutputs. These targets will probably be quite different by the end, but\nit's a good start to think about what it should look like in the end.\n\n-   All results for estimated relationships (in case we want to use it\n    for other output)\n-   Plot of statistical estimate for each relationship\n\nPotential function names might be:\n\n-   `calculate_estimates()`\n-   `plot_estimates()`\n\n\n\n\n```{mermaid}\n%%| label: fig-model-possible-targets\n%%| fig-cap: Potential inputs, outputs, and functions for the targets pipeline.\n%%| echo: false\n%%| eval: true\n%%{init:{'flowchart':{'nodeSpacing': 20, 'rankSpacing': 30}}}%%\ngraph TB\n    lipidomics -- \"calculate_estimates()\" --> model_est[Model estimates]\n    model_est -- \"plot_estimates()\" --> plot_est[Plot of estimates]\n    plot_est -- \"tar_read()\" --> qmd[Quarto]\n    \nlinkStyle 0,1,2 stroke-width:1px;\n```\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nA few things to repeat and reinforce:\n\n1.  The workflow of the image and that it all starts with the research\n    question.\n2.  The fact that almost all statistical methods are basically special\n    forms of linear regression.\n3.  That this model creation stage requires a variety of domain\n    expertise, not just statistical expertise.\n\nAlso repeat the question to ask, the outputs, as well as the functions\nand their flow that we can translate into the `{targets}` pipeline.\n:::\n\n## Defining the model\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nLet them read it first and then briefly verbally walk through this\nsection, describing the theoretical model both graphically and\nmathematically. Go through why we use `{tidymodels}` rather than other\napproaches.\n:::\n\n::: callout-note\n## Reading task: \\~10 minutes\n\nNow that we've talked about the workflow around making models and have\nalready written out some research questions, let's make a basic,\ngraphical theoretical model:\n\n\n\n\n```{mermaid}\n%%| label: fig-model-research-question\n%%| fig-cap: A simple theoretical model of the research question about T1D status.\n%%| echo: false\n%%| eval: true\n%%{init:{'flowchart':{'nodeSpacing': 20, 'rankSpacing':30}, 'themeVariables': { 'edgeLabelBackground': 'transparent'}}}%%\ngraph TB\n    Metabolite --> T1D\n    Age & Gender --> T1D & Metabolite\n\nlinkStyle 0,1,2,3,4 stroke-width:1px;\n```\n\n\n\n\nOr mathematically:\n\n$$T1D = metabolite + age + gender$$\n\nSo, T1D status (or `class` in the `lipidomics` dataset) is our\n**outcome** and the individual metabolite, age, and gender are our\n**predictors**. Technically, age and gender would be \"confounders\" or\n\"covariates\", since we include them only because we think they influence\nthe relationship between the metabolite and T1D.\n\nIf we convert the formula into a form with the variables we have in the\ndataset as well as selecting only one metabolite for now (the\ncholesterol metabolite, which we add \"metabolite\" to differentiate it\nfrom other potential variables), it would be:\n\n$$class = metabolite\\_cholesterol + age + gender$$\n\nNow that we have a theoretical model, we need to choose our model type.\nSince T1D is binary (either you have it or you don't), the most likely\nchoice is logistic regression, which requires a binary outcome variable.\nSo we have the theoretical model and the type of model to use - how do\nwe express this as code in R? There are many ways of doing the same\nthing in R, but some are a bit easier than others. One such approach,\nthat is quite generic and fits with the ideals of the `{tidyverse}`, is\na similar universe of packages called the `{tidymodels}`.\n\nWhy do we teach `{tidymodels}`? Because they are built by software\ndevelopers, employed by Posit (who also employs the people who build the\n`{tidyverse}` and RStudio), and they have a strong reputation for\nwriting good documentation. Plus, the `{tidymodels}` set of packages\nalso make creating and using models quite generic, so by teaching you\nthese sets of tools, you can relatively easily change the model type, or\nhow you process the data, or other specifications without having to\nlearn a whole new package or set of tools.\n\nThe reason `{tidymodels}` can do that is because they designed it in a\nway that makes a clear separation in the components of the model\nbuilding workflow that was described above, through the use of specific\npackages for each component.\n\n| Package       | Description                                                                                       |\n|---------------|---------------------------------------------------------------------------------------------------|\n| `{parsnip}`   | Model definition, such as type (e.g. `linear_reg()`) and \"engine\" (e.g. `glm()`).                 |\n| `{recipes}`   | Model-specific data transformations, such as removing missing values, or standardizing the data.  |\n| `{workflows}` | Combining model definition, data, and transformations to calculate the estimates and uncertainty. |\n\n: Core packages within `{tidymodels}`.\n\nWe'll start with the `{parsnip}` package. Functions in this package are\nused to set the details of the model you want to use. Specifically,\n[functions](https://parsnip.tidymodels.org/reference/index.html#models)\nto indicate the model *\"type\"* (e.g. linear regression) and the\n`set_engines()` function to determine the \"engine\" to run the type\n(which R-based algorithm to use, like `glm()` compared to `lm()`). Check\nout the\n[Examples](https://parsnip.tidymodels.org/articles/Examples.html) page\nfor code you might use depending on the model you want. The most\ncommonly used model types would be `linear_reg()`, `logistic_reg()`, and\n`multinom_reg()`.\n:::\n\nLet's switch to working in the `doc/learning.qmd` file to create those\nlogistic regression models. Since we've already created some items in\nthe `{targets}` pipeline, we'll need to tell the Quarto file where to\nfind this \"store\" of outputs, and import the data using `tar_read()`. We\nalso need to add `library(tidymodels)` to the `setup` code chunk. Copy\nand paste this code chunk below into the Quarto file.\n\n```{{r setup}}\ntargets::tar_config_set(store = here::here(\"_targets\"))\nlibrary(tidyverse)\nlibrary(targets)\nlibrary(tidymodels)\nsource(here::here(\"R/functions.R\"))\nlipidomics <- tar_read(lipidomics)\n```\n\nSince we will be using `{tidymodels}`, we need to install it, as well as\nexplicitly add the `{parsnip}`, `{recipes}`, and `{workflows}` packages.\nLike `{tidyverse}`, we need to set `{tidymodels}` differently because it\nis a \"meta-package\". We might need to force installing it with\n`pak::pak(\"tidymodels\")`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuse_package(\"tidymodels\", \"depends\")\n# pak::pak(\"tidymodels\")\nuse_package(\"parsnip\")\nuse_package(\"recipes\")\nuse_package(\"workflows\")\n```\n:::\n\n\n\n\nBefore continuing, let's **commit** the changes to the Git history with\n{{< var keybind.git >}}. Next, in the `doc/learning.qmd` file, on the\nbottom of the document create a new header and code chunk:\n\n````         \n## Building the model\n\n```{{r}}\n\n```\n````\n\nIn the new code chunk, we will set up the model specs:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlog_reg_specs <- logistic_reg() %>%\n  set_engine(\"glm\")\nlog_reg_specs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n```\n\n\n:::\n:::\n\n\n\n\nRunning this on it's own doesn't show much, as you can see. But we've\nnow set the model we want to use.\n\n## Data transformations specific to modeling\n\nSetting the model type was pretty easy right? The more difficult part\ncomes next with the data transformations. `{recipes}` functions are\nalmost entirely used to apply transformations that a model might\nspecifically need, like mean-centering, removing missing values, and\nother aspects of data processing.\n\nLet's consider our `lipidomics` dataset. In order for us to start our\nstatistical analysis, we need the data to be structured in a certain way\nto be able to smoothly use it as input in our model. We have at least\nthree easy observations on necessary transformations of the data, two of\nwhich can be fixed with a single `{tidyr}` function, while the third one\ncan be fixed with `{recipes}`. Can you spot them?\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nlipidomics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 504 × 6\n#>    code   gender   age class metabolite                value\n#>    <chr>  <chr>  <dbl> <chr> <chr>                     <dbl>\n#>  1 ERI109 M         25 CT    TMS (interntal standard) 208.  \n#>  2 ERI109 M         25 CT    Cholesterol               19.8 \n#>  3 ERI109 M         25 CT    Lipid CH3- 1              44.1 \n#>  4 ERI109 M         25 CT    Lipid CH3- 2             147.  \n#>  5 ERI109 M         25 CT    Cholesterol               27.2 \n#>  6 ERI109 M         25 CT    Lipid -CH2-              587.  \n#>  7 ERI109 M         25 CT    FA -CH2CH2COO-            31.6 \n#>  8 ERI109 M         25 CT    PUFA                      29.0 \n#>  9 ERI109 M         25 CT    Phosphatidylethanolamine   6.78\n#> 10 ERI109 M         25 CT    Phosphatidycholine        41.7 \n#> # ℹ 494 more rows\n```\n\n\n:::\n:::\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nAsk them if they can spot these differences. Give them a few minutes to\nthink and respond.\n:::\n\nThe first observation isn't always an issue and depends heavily on the\nmodel type you use. Since we are using logistic regression, the model\nassumes that each row is an individual person. But our data is in the\nlong format, so each person has multiple rows. The second observation is\nthat there seems to be a data input error, since there are three\n`Cholesterol` values, while all other metabolites only have one:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlipidomics %>%\n  count(code, metabolite) %>%\n  filter(n > 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 36 × 3\n#>    code   metabolite      n\n#>    <chr>  <chr>       <int>\n#>  1 ERI109 Cholesterol     3\n#>  2 ERI111 Cholesterol     3\n#>  3 ERI140 Cholesterol     3\n#>  4 ERI142 Cholesterol     3\n#>  5 ERI143 Cholesterol     3\n#>  6 ERI144 Cholesterol     3\n#>  7 ERI145 Cholesterol     3\n#>  8 ERI146 Cholesterol     3\n#>  9 ERI147 Cholesterol     3\n#> 10 ERI149 Cholesterol     3\n#> # ℹ 26 more rows\n```\n\n\n:::\n:::\n\n\n\n\nWe can fix both the long format and multiple cholesterol issues by using\n`tidyr::pivot_wider()`. Before we do, the last issue is that each\nmetabolite has quite large differences in the values and ranges of data.\nAgain, whether this is an issue depends on what we want to do, but in\nour research question we want to know how each metabolite influences\nT1D. In order to best interpret the results and compare across\nmetabolites, we should ideally have all the metabolites with a similar\nrange and distribution of values.\n\nLet's fix the first two issues first. While we probably only need to use\n`pivot_wider()`, we should probably first tidy up the metabolite names\nfirst so they make better column names. We do that by combining\n`mutate()` with `snakecase::to_snake_case()`. In the `doc/learning.qmd`\nfile, we rename the metabolite names before using `pivot_wider()`. Since\nwe want an easy way of identifying columns that are metabolites, we will\nadd a `\"metabolite_\"` prefix using the argument `names_prefix`. To\nactually fix the multiple cholesterol issue, we should look more into\nthe data documentation or contact the authors. But for this course, we\nwill merge the values by calculating a mean before pivoting. We do this\nby setting the `values_fn` with `mean`.\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nlipidomics_wide <- lipidomics %>%\n  mutate(metabolite = snakecase::to_snake_case(metabolite)) %>%\n  pivot_wider(\n    names_from = metabolite,\n    values_from = value,\n    values_fn = mean,\n    names_prefix = \"metabolite_\"\n  )\nlipidomics_wide\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 36 × 16\n#>    code   gender   age class metabolite_tms_interntal_s…¹ metabolite_cholesterol\n#>    <chr>  <chr>  <dbl> <chr>                        <dbl>                  <dbl>\n#>  1 ERI109 M         25 CT                           208.                   18.6 \n#>  2 ERI111 M         39 CT                           219.                   20.8 \n#>  3 ERI163 W         58 CT                            57.1                  15.5 \n#>  4 ERI375 M         24 CT                            19.2                  10.2 \n#>  5 ERI376 M         26 CT                            35.4                  13.5 \n#>  6 ERI391 M         31 CT                            30.4                   9.53\n#>  7 ERI392 M         24 CT                            21.7                   9.87\n#>  8 ERI79  W         26 CT                           185.                   17.6 \n#>  9 ERI81  M         52 CT                           207.                   17.0 \n#> 10 ERI83  M         25 CT                           322.                   19.7 \n#> # ℹ 26 more rows\n#> # ℹ abbreviated name: ¹​metabolite_tms_interntal_standard\n#> # ℹ 10 more variables: metabolite_lipid_ch_3_1 <dbl>,\n#> #   metabolite_lipid_ch_3_2 <dbl>, metabolite_lipid_ch_2 <dbl>,\n#> #   metabolite_fa_ch_2_ch_2_coo <dbl>, metabolite_pufa <dbl>,\n#> #   metabolite_phosphatidylethanolamine <dbl>,\n#> #   metabolite_phosphatidycholine <dbl>, metabolite_phospholipids <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n\nSince we're using a function-oriented workflow and since we will be\nusing this code again later on, let's convert both the \"metabolite to\nsnakecase\" and \"pivot to wider\" code into their own functions, before\nmoving them over into the `R/functions.R` file.\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\ncolumn_values_to_snake_case <- function(data) {\n  data %>%\n    dplyr::mutate(metabolite = snakecase::to_snake_case(metabolite))\n}\nlipidomics %>%\n  column_values_to_snake_case()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 504 × 6\n#>    code   gender   age class metabolite                value\n#>    <chr>  <chr>  <dbl> <chr> <chr>                     <dbl>\n#>  1 ERI109 M         25 CT    tms_interntal_standard   208.  \n#>  2 ERI109 M         25 CT    cholesterol               19.8 \n#>  3 ERI109 M         25 CT    lipid_ch_3_1              44.1 \n#>  4 ERI109 M         25 CT    lipid_ch_3_2             147.  \n#>  5 ERI109 M         25 CT    cholesterol               27.2 \n#>  6 ERI109 M         25 CT    lipid_ch_2               587.  \n#>  7 ERI109 M         25 CT    fa_ch_2_ch_2_coo          31.6 \n#>  8 ERI109 M         25 CT    pufa                      29.0 \n#>  9 ERI109 M         25 CT    phosphatidylethanolamine   6.78\n#> 10 ERI109 M         25 CT    phosphatidycholine        41.7 \n#> # ℹ 494 more rows\n```\n\n\n:::\n:::\n\n\n\n\nThis on its own should work. *However*, the column we want to change\nmight not always be called `metabolite`, or we might want to change it\nlater. So, to make this function a bit more generic, we can use\nsomething called \"curly-curly\" (it looks like `{{}}` when used) and\n\"non-standard evaluation\" (NSE).\n\n::: callout-note\n## Reading task: \\~10 minutes\n\nWhen you write your own functions that make use of functions in the\n`{tidyverse}`, you may eventually encounter an error that might not be\nvery easy to figure out. Here's a very simple example using `select()`,\nwhere one of your function's arguments is to select columns:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_nse <- function(data, columns) {\n  data %>%\n    dplyr::select(columns)\n}\n\nlipidomics %>%\n  test_nse(class)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\n#> Error in `dplyr::select()`:\n#> ! Can't select columns that don't exist.\n#> ✖ Column `columns` doesn't exist.\n```\n\n\n:::\n:::\n\n\n\n\nThe error occurs because of something called \"[non-standard\nevaluation](http://adv-r.had.co.nz/Computing-on-the-language.html)\" (or\nNSE). NSE is a major feature of R and is used quite a lot throughout R.\nNSE is used a lot in the `{tidyverse}` packages. It's one of the first\nthings computer scientists complain about when they use R, because it is\nnot a common thing in other programming languages. But NSE is what\nallows you to use formulas (e.g. `y ~ x + x2` in modeling, which we will\nshow shortly) or allows you to type out `select(class, age)` or\n`library(purrr)`. In \"standard evaluation\", these would instead be\n`select(\"Gender\", \"BMI\")` or `library(\"purrr\")`. So NSE gives\nflexibility and ease of use for the user (we don't have to type quotes\nevery time) when doing data analysis, but can give some headaches when\nprogramming in R, like when making functions. There's more detail about\nthis on the [dplyr\nwebsite](https://dplyr.tidyverse.org/articles/programming.html#warm-up),\nwhich lists some options to handle NSE while programming. The easiest\napproach is to wrap the argument with \"curly-curly\" (`{{}}`).\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_nse <- function(data, columns) {\n  data %>%\n    dplyr::select({{ columns }})\n}\n\nlipidomics %>%\n  test_nse(class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 504 × 1\n#>    class\n#>    <chr>\n#>  1 CT   \n#>  2 CT   \n#>  3 CT   \n#>  4 CT   \n#>  5 CT   \n#>  6 CT   \n#>  7 CT   \n#>  8 CT   \n#>  9 CT   \n#> 10 CT   \n#> # ℹ 494 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nlipidomics %>%\n  test_nse(c(class, age))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 504 × 2\n#>    class   age\n#>    <chr> <dbl>\n#>  1 CT       25\n#>  2 CT       25\n#>  3 CT       25\n#>  4 CT       25\n#>  5 CT       25\n#>  6 CT       25\n#>  7 CT       25\n#>  8 CT       25\n#>  9 CT       25\n#> 10 CT       25\n#> # ℹ 494 more rows\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nYou don't need to go over what they read, you can continue with making\nthe function below. Unless learners have some questions.\n:::\n\nWe can use curly-curly (combined with `across()`) to apply\n`snakecase::to_snake_case()` to columns of our choice.\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\ncolumn_values_to_snake_case <- function(data, columns) {\n  data %>%\n    dplyr::mutate(dplyr::across({{ columns }}, snakecase::to_snake_case))\n}\n\nlipidomics %>%\n  column_values_to_snake_case(metabolite)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 504 × 6\n#>    code   gender   age class metabolite                value\n#>    <chr>  <chr>  <dbl> <chr> <chr>                     <dbl>\n#>  1 ERI109 M         25 CT    tms_interntal_standard   208.  \n#>  2 ERI109 M         25 CT    cholesterol               19.8 \n#>  3 ERI109 M         25 CT    lipid_ch_3_1              44.1 \n#>  4 ERI109 M         25 CT    lipid_ch_3_2             147.  \n#>  5 ERI109 M         25 CT    cholesterol               27.2 \n#>  6 ERI109 M         25 CT    lipid_ch_2               587.  \n#>  7 ERI109 M         25 CT    fa_ch_2_ch_2_coo          31.6 \n#>  8 ERI109 M         25 CT    pufa                      29.0 \n#>  9 ERI109 M         25 CT    phosphatidylethanolamine   6.78\n#> 10 ERI109 M         25 CT    phosphatidycholine        41.7 \n#> # ℹ 494 more rows\n```\n\n\n:::\n:::\n\n\n\n\nMove this new function over into the `R/functions.R` file, add Roxygen\ndocumentation with {{< var keybind.roxygen >}}, style using\n{{< var keybind.styler >}}, `source()` the modified `R/functions.R` file\nwith {{< var keybind.source >}}, and add the new function above the\n`pivot_wider()` code in the `doc/learning.qmd` file.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#' Convert a column's character values to snakecase format.\n#'\n#' @param data The lipidomics dataset.\n#' @param columns The column you want to convert into the snakecase format.\n#'\n#' @return A data frame.\n#'\ncolumn_values_to_snake_case <- function(data, columns) {\n  data %>%\n    dplyr::mutate(dplyr::across({{ columns }}, snakecase::to_snake_case))\n}\n```\n:::\n\n\n\n\n## Exercise: Convert the pivot code into a function\n\n> Time: \\~10 minutes.\n\nJust like with the `mutate()`, take the `pivot_wider()` code and convert\nit into a new function.\n\n1.  Name the new function `metabolites_to_wider`.\n2.  Include one argument in the new `function()`: `data`.\n3.  Use `data %>%` at the beginning, like we did with the\n    `column_values_to_snake_case()`.\n4.  Use `tidyr::` before the `pivot_wider()` function.\n5.  Add the Roxygen documentation with {{< var keybind.roxygen >}}.\n6.  Move the function into the `R/functions.R` file.\n7.  Replace the code in the `doc/learning.qmd` file to make use of the\n    new functions.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"**Click for the solution**. Only click if you are struggling or are out of time.\"}\n#' Convert the metabolite long format into a wider one.\n#'\n#' @param data The lipidomics dataset.\n#'\n#' @return A wide data frame.\n#'\nmetabolites_to_wider <- function(data) {\n  data %>%\n    tidyr::pivot_wider(\n      names_from = metabolite,\n      values_from = value,\n      values_fn = mean,\n      names_prefix = \"metabolite_\"\n    )\n}\n```\n:::\n\n\n\n\n## Using recipes to manage transformations\n\nWe've used `{dplyr}` and `{tidyr}` to start fixing some of the issues\nwith the data. But we still have the third issue: How to make the\nresults between metabolites comparable. That's where we use `{recipes}`.\n\nThe first function is `recipe()` and it takes two forms: with or without\na formula. Remember the model formula we mentioned previously? Well,\nhere is where we can use it to tell `{recipes}` about the model formula\nwe intend to use so it knows on what variables to apply the chosen\ntransformations.\n\nFor a few reasons that will be clearer later, we won't ultimately use\nthe formula form of `recipe()`, but will show how it works. The\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe(class ~ metabolite_cholesterol + age + gender, data = lipidomics_wide)\n```\n:::\n\n\n\n\nThe alternative approach is to set \"roles\" using `update_roles()`.\nInstead of using a formula and letting `recipe()` infer the outcome and\npredictors, we can explicitly select which variables are which. This has\nsome nice features that we will use later on.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe(lipidomics_wide) %>%\n  update_role(metabolite_cholesterol, age, gender, new_role = \"predictor\") %>%\n  update_role(class, new_role = \"outcome\")\n```\n:::\n\n\n\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nDescribe this next paragraph by switching to this website and showing\nthe list of some of the steps available in the code chunk below.\n:::\n\nThe next \"step\" is to select a transformation function. There are many\n`step_*` functions (some shown below) that are available in `{recipes}`\nand which one you decide to use depends heavily on your data and your\nresearch question. So take your time thinking through which\ntransformations you actually need for your analysis, rather than quickly\nusing one or more of them.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipes::step_log()\nrecipes::step_scale()\nrecipes::step_normalize()\nrecipes::step_center()\nrecipes::step_sqrt()\n```\n:::\n\n\n\n\n::: {.callout-tip appearance=\"default\"}\nThere are so many useful transformation functions available. For\ninstance, if you often have to impute data, there are functions for\nthat. You can check them out in the Console by typing\n`recipes::step_impute_` then hit the Tab key to see a list of them. Or,\nif you have some missing values, there's also the\n`recipes::step_naomit()`.\n:::\n\nThere are many transformations we could use for the `lipidomics`\ndataset, but we will use `step_normalize()` for this course. This\nfunction is useful because it makes each variable centered to zero and a\nvalue of 1 unit is translated to 1 standard deviation of the original\ndistribution. This means we can more easily compare values between\nvariables. We can add this to the end of the recipe:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe(lipidomics_wide) %>%\n  update_role(metabolite_cholesterol, age, gender, new_role = \"predictor\") %>%\n  update_role(class, new_role = \"outcome\") %>%\n  step_normalize(starts_with(\"metabolite_\"))\n```\n:::\n\n\n\n\nNext thing to do is convert this into a function, using the same\nworkflow we've been using (which means this needs to be in the\n`R/functions.R` script). We'll also use the curly-curly again, since we\nmight use a different metabolite later. Note, when adding all the\n`packagename::` to each function, the `starts_with()` function comes\nfrom the `{tidyselect}` package.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#' A transformation recipe to pre-process the data.\n#'\n#' @param data The lipidomics dataset.\n#' @param metabolite_variable The column of the metabolite variable.\n#'\n#' @return\n#'\ncreate_recipe_spec <- function(data, metabolite_variable) {\n  recipes::recipe(data) %>%\n    recipes::update_role({{ metabolite_variable }}, age, gender, new_role = \"predictor\") %>%\n    recipes::update_role(class, new_role = \"outcome\") %>%\n    recipes::step_normalize(tidyselect::starts_with(\"metabolite_\"))\n}\n```\n:::\n\n\n\n\nAnd test it out:\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe_specs <- lipidomics_wide %>%\n  create_recipe_spec(metabolite_cholesterol)\nrecipe_specs\n```\n:::\n\n\n\n\nStyle using {{< var keybind.styler >}}, then commit the changes made to\nthe Git history with {{< var keybind.git >}}.\n\n## Fitting the model by combining the recipe, model definition, and data {#sec-fitting-model}\n\nWe've now defined the model we want to use and created a transformation\n`{recipes}` specification. Now we can start putting them together and\nfinally fit them to the data. This is done with the `{workflows}`\npackage.\n\nWhy use this package, rather than simply run the statistical analysis\nand process the data as normal? When running multiple models (like we\nwill do in the next section) that may require different data structures,\nthe data transformation steps have to happen *right before* the data is\nfit to the model and need to be done on exactly the data used by the\nmodel. So if we have one data frame that we run multiple models on, but\nthe transformation happens to the whole data frame, we could end up with\nissues due to how the transformations were applied. The `{workflows}`\npackage keeps track of those things for us, so we can focus on the\nhigher level thinking rather than on the small details of running the\nmodels.\n\nThe `{workflows}` package has a few main functions for combining the\nrecipe with the model specs, as well as several for updating an existing\nworkflow (which might be useful if you need to run many models of\nslightly different types). All model workflows need to start with\n`workflow()`, followed by two main functions: `add_model()` and\n`add_recipe()`. Can you guess what they do?\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nworkflow() %>%\n  add_model(log_reg_specs) %>%\n  add_recipe(recipe_specs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ══ Workflow ════════════════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> 1 Recipe Step\n#> \n#> • step_normalize()\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n```\n\n\n:::\n:::\n\n\n\n\nWhile this code is already pretty concise, let's convert it into a\nfunction to make it simplified. We'll use the same function-oriented\nworkflow that we've used before, where the function should ultimately be\ninside the `R/functions.R` file.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#' Create a workflow object of the model and transformations.\n#'\n#' @param model_specs The model specs\n#' @param recipe_specs The recipe specs\n#'\n#' @return A workflow object\n#'\ncreate_model_workflow <- function(model_specs, recipe_specs) {\n  workflows::workflow() %>%\n    workflows::add_model(model_specs) %>%\n    workflows::add_recipe(recipe_specs)\n}\n```\n:::\n\n\n\n\n<!-- TODO: image showing pieces at play that we need to eventually fit into targets/questions -->\n\nInstead of using the previously created objects, let's start the model\ncreation from scratch:\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_workflow <- create_model_workflow(\n  logistic_reg() %>%\n    set_engine(\"glm\"),\n  lipidomics_wide %>%\n    create_recipe_spec(metabolite_cholesterol)\n)\nmodel_workflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ══ Workflow ════════════════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> 1 Recipe Step\n#> \n#> • step_normalize()\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> Logistic Regression Model Specification (classification)\n#> \n#> Computational engine: glm\n```\n\n\n:::\n:::\n\n\n\n\nNow, we can do the final thing: Fitting the data to the model with\n`fit()`! :raised_hands:\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nfitted_model <- model_workflow %>%\n  fit(lipidomics_wide)\nfitted_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: logistic_reg()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> 1 Recipe Step\n#> \n#> • step_normalize()\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> \n#> Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n#> \n#> Coefficients:\n#>            (Intercept)                 genderW                     age  \n#>               0.105293               -0.706550                0.006902  \n#> metabolite_cholesterol  \n#>               1.087989  \n#> \n#> Degrees of Freedom: 35 Total (i.e. Null);  32 Residual\n#> Null Deviance:\t    49.91 \n#> Residual Deviance: 42.8 \tAIC: 50.8\n```\n\n\n:::\n:::\n\n\n\n\nThis gives us a lot of information, but what we are mostly interested in\nis the model estimates themselves. While this `fitted_model` object\ncontains a lot additional information inside, `{workflows}` thankfully\nhas a function to extract the information we want. In this case, it is\nthe `extract_fit_parsnip()` function.\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nfitted_model %>%\n  extract_fit_parsnip()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> parsnip model object\n#> \n#> \n#> Call:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n#> \n#> Coefficients:\n#>            (Intercept)                 genderW                     age  \n#>               0.105293               -0.706550                0.006902  \n#> metabolite_cholesterol  \n#>               1.087989  \n#> \n#> Degrees of Freedom: 35 Total (i.e. Null);  32 Residual\n#> Null Deviance:\t    49.91 \n#> Residual Deviance: 42.8 \tAIC: 50.8\n```\n\n\n:::\n:::\n\n\n\n\nTo get this information in a tidier format, we use another function:\n`tidy()`. This function comes from the `{broom}` package, which is part\nof the `{tidymodels}`. But we should explicitly add it to the\ndependencies:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuse_package(\"broom\")\n```\n:::\n\n\n\n\nThen, we add the `tidy()` function to our model using the `%>%` pipe.\nSince we are using a logistic regression model, we need to consider how\nwe want the estimates to be presented, probably depending on how we want\nto visualize our results. If we set `exponentiate = TRUE` in `tidy()`,\nthe output estimates will be odds ratios, if we set\n`exponentiate = FALSE`, we will get the log odds ratios or the beta\ncoefficient. Here we choose `exponentiate = TRUE`:\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nfitted_model %>%\n  extract_fit_parsnip() %>%\n  tidy(exponentiate = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 5\n#>   term                   estimate std.error statistic p.value\n#>   <chr>                     <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)               1.11     1.29      0.0817  0.935 \n#> 2 genderW                   0.493    0.779    -0.907   0.365 \n#> 3 age                       1.01     0.0377    0.183   0.855 \n#> 4 metabolite_cholesterol    2.97     0.458     2.38    0.0175\n```\n\n\n:::\n:::\n\n\n\n\nWe now have a data frame of our model results! Like we did with the\n`workflows()` code that we converted into a function, we do the same\nthing here: Make another function (and move it to `R/functions.R`)!\n:stuck_out_tongue:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#' Create a tidy output of the model results.\n#'\n#' @param workflow_fitted_model The model workflow object that has been fitted.\n#'\n#' @return A data frame.\n#'\ntidy_model_output <- function(workflow_fitted_model) {\n  workflow_fitted_model %>%\n    workflows::extract_fit_parsnip() %>%\n    broom::tidy(exponentiate = TRUE)\n}\n```\n:::\n\n\n\n\nReplacing the code in the `doc/learning.qmd` file to use the function.\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\nfitted_model %>%\n  tidy_model_output()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 5\n#>   term                   estimate std.error statistic p.value\n#>   <chr>                     <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)               1.11     1.29      0.0817  0.935 \n#> 2 genderW                   0.493    0.779    -0.907   0.365 \n#> 3 age                       1.01     0.0377    0.183   0.855 \n#> 4 metabolite_cholesterol    2.97     0.458     2.38    0.0175\n```\n\n\n:::\n:::\n\n\n\n\nIf we revise the code so it is one pipe, it would look like:\n\n\n\n\n::: {.cell .column-page-inset-right layout-align=\"center\"}\n\n```{.r .cell-code}\ncreate_model_workflow(\n  logistic_reg() %>%\n    set_engine(\"glm\"),\n  lipidomics_wide %>%\n    create_recipe_spec(metabolite_cholesterol)\n) %>%\n  fit(lipidomics_wide) %>%\n  tidy_model_output()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 4 × 5\n#>   term                   estimate std.error statistic p.value\n#>   <chr>                     <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)               1.11     1.29      0.0817  0.935 \n#> 2 genderW                   0.493    0.779    -0.907   0.365 \n#> 3 age                       1.01     0.0377    0.183   0.855 \n#> 4 metabolite_cholesterol    2.97     0.458     2.38    0.0175\n```\n\n\n:::\n:::\n\n\n\n\nLet's briefly cover what these columns and values mean.\n\n::: {.callout-note collapse=\"true\"}\n## Instructor note\n\nIf you want, you can go over these details briefly or in more detail,\ndepending on how comfortable you are. Or you can get them to read it\nonly.\n:::\n\n::: callout-note\n## Reading task: \\~10 minutes\n\nLet's explain this output a bit, each column at a time:\n\n-   `term`: If you recall the formula\n    $class = metabolite + sex + gender$, you'll see all but the `class`\n    object there in the column `term`. This column contains all the\n    predictor variables, including the intercept (from the original\n    model).\n\n-   `estimate`: This column is the \"coefficient\" linked to the term in\n    the model. The final mathematical model here looks like:\n\n    $$ \\displaylines{class = Intercept + (metabolite\\_estimate \\times metabolite\\_value) + \\\\ (gender\\_estimate \\times gender\\_value) + ...}$$\n\n    In our example, we chose to get the odds ratios. In the mathematical\n    model above, the estimate is represented as the log odds ratio or\n    beta coefficient - the constant value you multiply the value of the\n    term with. Interpreting each of these values can be quite tricky and\n    can take a surprising amount of time to conceptually break down, so\n    we won't do that here, since this isn't a statistics course. The\n    only thing you need to understand here is that the `estimate` is the\n    value that tells us the *magnitude* of association between the term\n    and `class`. This value, along with the `std.error` are the most\n    important values we can get from the model and we will be using them\n    when presenting the results.\n\n-   `std.error`: This is the uncertainty in the `estimate` value. A\n    higher value means there is less certainty in the value of the\n    `estimate`.\n\n-   `statistic`: This value is used to, essentially, calculate the\n    `p.value`.\n\n-   `p.value`: This is the infamous value we researchers go crazy for\n    and think nothing else of. While there is a lot of attention to this\n    single value, we tend to give it more attention than warranted. The\n    interpretation of the p-value is even more difficult than the\n    `estimate` and again, we won't cover this in this course. We won't\n    be using this value at all in presenting the results.\n:::\n\nBefore ending, open the Git interface and commit the changes you made\nwith {{< var keybind.git >}}. Then push your changes up to GitHub.\n\n## Summary\n\n-   Create research questions that (ideally) are structured in a way to\n    mimic how the statistical analysis will be done, preferably in a\n    \"formula\" style like $y = x1 + x2 + ... + error$ and in a diagram\n    style with links connecting variables.\n-   Statistical analyses, while requiring some trial and error, are\n    surprisingly structured in the workflow and steps taken. Use this\n    structure to help guide you in completing tasks related to running\n    analyses.\n-   Use `{parsnip}` functions to define the model you want to use, like\n    `logistic_reg()` for logistic regression, and set the computational\n    \"engine\" with `set_engine()`.\n-   Use `{recipes}` functions to set up the data transformation steps\n    necessary to effectively run the statistical analysis, like adding\n    variable \"roles\" (outcome vs predictor) using `update_roles()` and\n    adding transformation steps using any of the dozen different `step_`\n    functions.\n-   Use `{workflows}` functions to develop an analysis `workflow()` that\n    combines the defined model with `add_model()`, the transformation\n    steps with `add_recipe()`, and the data with `fit()`.\n-   Use `{broom}` to `tidy()` the model output, extracted using\n    `extract_fit_parsnip()` to get a data frame of the estimates and\n    standard error for the variables in the model.\n",
    "supporting": [
      "stats-analyses-basic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}